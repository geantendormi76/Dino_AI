# training_pipelines/3_policy_pipeline/3_train_and_export.py (L3+ ç‰ˆæœ¬)
import sys
from pathlib import Path
import numpy as np
import torch
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import torch.nn as nn

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.brain.decision_transformer import DecisionTransformer

# --- æ ¸å¿ƒé…ç½® ---
DATASET_PATH = project_root / "data" / "policy_data" / "processed" / "dino_dt_dataset.npz"
MODEL_SAVE_PATH = project_root / "models" / "policy"
MODEL_SAVE_PATH.mkdir(exist_ok=True, parents=True)
ONNX_MODEL_PATH = MODEL_SAVE_PATH / "dino_decision_transformer.onnx"

# --- PyTorch æ•°æ®é›†ç±» ---
class TrajectoryDataset(Dataset):
    def __init__(self, data, context_len):
        self.data = data
        self.context_len = context_len

    def __len__(self):
        return len(self.data['actions']) - self.context_len

    def __getitem__(self, idx):
        end_idx = idx + self.context_len
        return {
            'arena_grids': torch.from_numpy(self.data['arena_grids'][idx:end_idx]),
            'global_features': torch.from_numpy(self.data['global_features'][idx:end_idx]),
            'actions': torch.from_numpy(self.data['actions'][idx:end_idx]),
            'rtgs': torch.from_numpy(self.data['rtgs'][idx:end_idx]).unsqueeze(-1),
            'timesteps': torch.from_numpy(self.data['timesteps'][idx:end_idx]).unsqueeze(-1)
        }

def main():
    print("ğŸ§  å¼€å§‹è®­ç»ƒå†³ç­–Transformer...")
    
    # 1. æ¨¡å‹é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    config = {
        'state_channels': 2, # æ¥è‡ª StateRepresentationBuilder
        'action_dim': 1,     # åŠ¨ä½œæ˜¯ä¸€ä¸ªå•ä¸€çš„æ•´æ•° (0, 1, 2)
        'num_actions': 3,    # åŠ¨ä½œç©ºé—´çš„ç±»åˆ«æ•°
        'hidden_size': 128,
        'n_layer': 3,
        'n_head': 4,
        'dropout': 0.1,
        'max_len': 20, # å†³ç­–æ—¶å‚è€ƒçš„ä¸Šä¸‹æ–‡é•¿åº¦
        'device': device
    }

    # 2. åŠ è½½æ•°æ®é›†
    try:
        data = np.load(DATASET_PATH)
        dataset = TrajectoryDataset(data, context_len=config['max_len'])
        dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4)
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {DATASET_PATH}")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {DATASET_PATH}ã€‚è¯·å…ˆè¿è¡Œ 2_process_data.pyã€‚")
        return
        
    # 3. åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    model = DecisionTransformer(config).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=1e-4)
    loss_fn = nn.CrossEntropyLoss()

    # 4. è®­ç»ƒå¾ªç¯
    num_epochs = 10
    for epoch in range(num_epochs):
        print(f"--- Epoch {epoch+1}/{num_epochs} ---")
        model.train()
        total_loss = 0
        for batch in tqdm(dataloader, desc="è®­ç»ƒä¸­"):
            optimizer.zero_grad()

            states = {
                'arena_grid': batch['arena_grids'].to(device),
                'global_features': batch['global_features'].to(device)
            }
            # å°†ç¦»æ•£åŠ¨ä½œè½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç  (One-Hot)
            actions = nn.functional.one_hot(batch['actions'], num_classes=config['action_dim']).float().to(device)
            rtgs = batch['rtgs'].to(device)
            timesteps = batch['timesteps'].to(device)
            
            # è·å–æ¨¡å‹é¢„æµ‹
            action_preds = model(states, actions, rtgs, timesteps) # (B, T, num_actions)

            # è®¡ç®—æŸå¤±
            # æˆ‘ä»¬åªé¢„æµ‹ä¸‹ä¸€ä¸ªåŠ¨ä½œï¼Œæ‰€ä»¥ç›®æ ‡æ˜¯ action_preds çš„è¾“å‡º
            # çœŸå®åŠ¨ä½œæ˜¯è¾“å…¥çš„ actions
            action_targets = batch['actions'].to(device) # (B, T)
            
            # Reshape for CrossEntropyLoss: (B * T, num_actions) and (B * T)
            loss = loss_fn(action_preds.reshape(-1, config['num_actions']), action_targets.reshape(-1))
            
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # 5. ä¿å­˜å¹¶å¯¼å‡ºæ¨¡å‹
    print(f"ğŸ’¾ è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨å°†æœ€ç»ˆç­–ç•¥å¯¼å‡ºä¸º ONNX æ ¼å¼åˆ° {ONNX_MODEL_PATH}...")
    torch.save(model.state_dict(), str(MODEL_SAVE_PATH / "dino_dt_model.pth"))
    
    # å¯¼å‡ºONNXéœ€è¦ä¸€ä¸ªå‡çš„è¾“å…¥
    dummy_states = {
        'arena_grid': torch.randn(1, config['max_len'], 2, 25, 80).to(device),
        'global_features': torch.randn(1, config['max_len'], 1).to(device)
    }
    dummy_actions = torch.zeros(1, config['max_len'], config['action_dim']).to(device)
    dummy_rtgs = torch.randn(1, config['max_len'], 1).to(device)
    dummy_timesteps = torch.zeros(1, config['max_len'], 1, dtype=torch.long).to(device)
    
    torch.onnx.export(
        model,
        (dummy_states, dummy_actions, dummy_rtgs, dummy_timesteps),
        str(ONNX_MODEL_PATH),
        export_params=True,
        opset_version=12,
        do_constant_folding=True,
        input_names=['arena_grids', 'global_features', 'actions', 'rtgs', 'timesteps'],
        output_names=['output_actions'],
    )
    print("\n" + "="*50)
    print("ğŸ‰ğŸ‰ğŸ‰ å†³ç­–Transformerå¤§è„‘å·²æˆåŠŸé“¸é€ ï¼ ğŸ‰ğŸ‰ğŸ‰")
    print(f"æœ€ç»ˆäº§å‡º: {ONNX_MODEL_PATH}")
    print("="*50)

if __name__ == "__main__":
    main()