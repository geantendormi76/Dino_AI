=== æ ¸å¿ƒç›®å½•ç»“æ„æ ‘ (ä»…æ–‡ä»¶å¤¹å’Œ.pyæ–‡ä»¶): ===

C:\dino_ai
|   |-- pyç»“æ„æ ‘.py
|   |-- run_bot.py
    |-- data
        |-- classification_data
            |-- bird-style1-02
            |-- bird-style1-03
            |-- cactus-large_double
            |-- cactus-large_single
            |-- cactus-small_double
            |-- cactus-small_single
            |-- cactus-small_triple
        |-- detection_data
            |-- images
            |-- labels
        |-- policy_data
            |-- processed
            |-- raw
    |-- models
        |-- classification
        |-- detection
            |-- onnx_cache
        |-- onnx_cache
        |-- policy
    |-- src
    |   |-- __init__.py
    |   |-- brain.py
    |   |-- config.py
    |   |-- contracts.py
    |   |-- state_builder.py
        |-- controls
        |   |-- __init__.py
        |   |-- agent.py
        |-- perception
        |   |-- __init__.py
        |   |-- detector.py
        |-- utils
        |   |-- __init__.py
        |   |-- screen_manager.py
        |-- world_modeling
        |   |-- __init__.py
        |   |-- state.py
        |   |-- world_model.py
    |-- training_pipelines
        |-- 1_detection_pipeline
        |   |-- train_detector.py
        |-- 2_classification_pipeline
        |   |-- 1_generate_dataset.py
        |   |-- 2_train_classifier.py
        |-- 3_policy_pipeline
        |   |-- 1_collect_data.py
        |   |-- 2_process_data.py
        |   |-- 3_train_and_export.py
    |-- _assets
    |-- _inputs
        |-- base_models
    |-- è®­ç»ƒæ•°æ®å›¾

=== æ ¸å¿ƒPythonæ–‡ä»¶ (.py) åˆ—è¡¨ (ç»å¯¹è·¯å¾„): ===

C:\dino_ai\pyç»“æ„æ ‘.py
C:\dino_ai\run_bot.py
C:\dino_ai\src\__init__.py
C:\dino_ai\src\brain.py
C:\dino_ai\src\config.py
C:\dino_ai\src\contracts.py
C:\dino_ai\src\controls\__init__.py
C:\dino_ai\src\controls\agent.py
C:\dino_ai\src\perception\__init__.py
C:\dino_ai\src\perception\detector.py
C:\dino_ai\src\state_builder.py
C:\dino_ai\src\utils\__init__.py
C:\dino_ai\src\utils\screen_manager.py
C:\dino_ai\src\world_modeling\__init__.py
C:\dino_ai\src\world_modeling\state.py
C:\dino_ai\src\world_modeling\world_model.py
C:\dino_ai\training_pipelines\1_detection_pipeline\train_detector.py
C:\dino_ai\training_pipelines\2_classification_pipeline\1_generate_dataset.py
C:\dino_ai\training_pipelines\2_classification_pipeline\2_train_classifier.py
C:\dino_ai\training_pipelines\3_policy_pipeline\1_collect_data.py
C:\dino_ai\training_pipelines\3_policy_pipeline\2_process_data.py
C:\dino_ai\training_pipelines\3_policy_pipeline\3_train_and_export.py

=== æ ¸å¿ƒPythonæ–‡ä»¶ä»£ç å†…å®¹: ===


--- æ–‡ä»¶: C:\dino_ai\pyç»“æ„æ ‘.py ---

import os
import sys

def generate_project_info(root_dir, output_filename="project_info.txt"):
    """
    ç”Ÿæˆé¡¹ç›®ä¿¡æ¯æŠ¥å‘Šï¼ŒåŒ…æ‹¬ç›®å½•æ ‘å’ŒPythonæ–‡ä»¶å†…å®¹ã€‚
    è¯¥æŠ¥å‘Šä¼šè¿‡æ»¤æ‰è™šæ‹Ÿç¯å¢ƒã€ç¼“å­˜ç›®å½•å’Œå¸¸è§çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œ
    åªä¿ç•™å¯¹AIåˆ†æé¡¹ç›®æ ¸å¿ƒæœ‰ç”¨çš„ä¿¡æ¯ã€‚

    Args:
        root_dir (str): è¦åˆ†æçš„æ ¹ç›®å½•è·¯å¾„ã€‚
        output_filename (str): æŠ¥å‘Šè¾“å‡ºçš„æ–‡ä»¶åã€‚
    """
    try:
        # ç¡®ä¿æ ¹ç›®å½•å­˜åœ¨
        if not os.path.isdir(root_dir):
            print(f"é”™è¯¯ï¼šæŒ‡å®šçš„è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•: {root_dir}")
            return

        # å®šä¹‰è¦å¿½ç•¥çš„ç›®å½•å’Œæ–‡ä»¶æ¨¡å¼
        ignored_dirs = [
            'venv', '.venv', 'env', '__pycache__', '.git', '.idea',
            'node_modules', 'dist', 'build', '.vscode', '.pytest_cache',
            '__pycache__', 'site-packages' # å¸¸è§Pythonè™šæ‹Ÿç¯å¢ƒå’Œç¼“å­˜ç›®å½•
        ]
        # å®šä¹‰è¦å¿½ç•¥çš„æ–‡ä»¶æ‰©å±•åï¼ˆäºŒè¿›åˆ¶æ–‡ä»¶ã€æ—¥å¿—ã€é…ç½®æ–‡ä»¶ç­‰ï¼‰
        ignored_extensions = [
            '.log', '.bin', '.dll', '.exe', '.so', '.dylib', '.DS_Store',
            '.pyc', '.pyo', '.ipynb_checkpoints', '.tmp', '.bak', '.swp',
            '.sqlite3', '.db', '.dat', '.json', '.yaml', '.yml', '.toml',
            '.xml', '.txt', '.md', '.csv', '.xlsx', '.xls', '.pdf',
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.ico',
            '.zip', '.tar', '.gz', '.rar', '.7z', '.mp3', '.mp4', '.avi',
            # æ‚¨å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“é¡¹ç›®è¿›ä¸€æ­¥æ‰©å±•æ­¤åˆ—è¡¨
        ]

        with open(output_filename, 'w', encoding='utf-8') as f:

            # === 1. ç”Ÿæˆç›®å½•ç»“æ„æ ‘ (ä»…æ ¸å¿ƒæ–‡ä»¶å¤¹å’Œ.pyæ–‡ä»¶) ===
            f.write("=== æ ¸å¿ƒç›®å½•ç»“æ„æ ‘ (ä»…æ–‡ä»¶å¤¹å’Œ.pyæ–‡ä»¶): ===\n")
            f.write("\n")

            # å­˜å‚¨æ‰€æœ‰.pyæ–‡ä»¶çš„è·¯å¾„ï¼Œä»¥ä¾¿åç»­è¯»å–å†…å®¹
            py_files_paths = []

            for dirpath, dirnames, filenames in os.walk(root_dir):
                # è¿‡æ»¤æ‰è¦å¿½ç•¥çš„ç›®å½•
                dirnames[:] = [d for d in dirnames if d not in ignored_dirs]

                # æ£€æŸ¥å½“å‰ç›®å½•æ˜¯å¦ä¸ºè¢«å¿½ç•¥çš„ç›®å½•æœ¬èº«
                if os.path.basename(dirpath) in ignored_dirs and dirpath != root_dir:
                    continue

                # è®¡ç®—å½“å‰ç›®å½•çš„æ·±åº¦ï¼Œç”¨äºæ ‘å½¢ç¼©è¿›
                depth = dirpath[len(root_dir):].count(os.sep)
                indent = '    ' * depth
                
                # å†™å…¥å½“å‰ç›®å½•å
                if dirpath == root_dir:
                    f.write(f"{root_dir}\n")
                else:
                    f.write(f"{indent}|-- {os.path.basename(dirpath)}\n")

                # å†™å…¥ .py æ–‡ä»¶ (è¿‡æ»¤æ‰å¿½ç•¥çš„æ–‡ä»¶æ‰©å±•å)
                for filename in sorted(filenames):
                    if filename.endswith(".py") and not any(filename.endswith(ext) for ext in ignored_extensions):
                        f.write(f"{indent}|   |-- {filename}\n")
                        py_files_paths.append(os.path.join(dirpath, filename))
            f.write("\n")

            # === 2. åˆ—å‡ºæ‰€æœ‰æ ¸å¿ƒPythonæ–‡ä»¶ (.py) åˆ—è¡¨ ===
            f.write("=== æ ¸å¿ƒPythonæ–‡ä»¶ (.py) åˆ—è¡¨ (ç»å¯¹è·¯å¾„): ===\n")
            f.write("\n")
            if py_files_paths:
                for py_file_path in sorted(py_files_paths):
                    f.write(f"{py_file_path}\n")
            else:
                f.write("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ ¸å¿ƒ.pyæ–‡ä»¶ã€‚\n")
            f.write("\n")

            # === 3. è·å–Pythonæ–‡ä»¶ä»£ç å†…å®¹ ===
            f.write("=== æ ¸å¿ƒPythonæ–‡ä»¶ä»£ç å†…å®¹: ===\n")
            f.write("\n")

            if py_files_paths:
                for py_file_path in sorted(py_files_paths):
                    f.write(f"\n--- æ–‡ä»¶: {py_file_path} ---\n")
                    f.write("\n")
                    try:
                        with open(py_file_path, 'r', encoding='utf-8') as py_f:
                            f.write(py_f.read())
                        f.write("\n")
                        f.write("----------------------------------------------------\n")
                    except Exception as e:
                        f.write(f"æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹: {e}\n")
                        f.write("----------------------------------------------------\n")
            else:
                f.write("æ²¡æœ‰å¯è¯»å–å†…å®¹çš„.pyæ–‡ä»¶ã€‚\n")

            f.write("\n")
            f.write("====================================================\n")
            f.write("  [AI åˆ†ææ•°æ®ç»“æŸ]\n")
            f.write("====================================================\n")

        print(f"æŠ¥å‘Šå·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜åˆ°: {output_filename}")
        print("æ“ä½œå®Œæˆã€‚")

    except Exception as e:
        print(f"å‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯: {e}")

if __name__ == "__main__":
    # è·å–è„šæœ¬æ‰€åœ¨çš„ç›®å½•ä½œä¸ºåˆ†æçš„æ ¹ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å…è®¸ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šè·¯å¾„ï¼Œå¦åˆ™ä½¿ç”¨è„šæœ¬æ‰€åœ¨ç›®å½•
    if len(sys.argv) > 1:
        input_path = sys.argv[1]
        # å¯¹ç”¨æˆ·è¾“å…¥çš„è·¯å¾„è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œä»¥åº”å¯¹Windowså’ŒWSLè·¯å¾„çš„æ··åˆæƒ…å†µ
        # os.path.abspath å¯ä»¥å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        # os.path.normpath å¯ä»¥è§„èŒƒåŒ–è·¯å¾„ï¼Œå¤„ç†'..' '.' ç­‰
        # æ³¨æ„ï¼šå¯¹äº \\wsl.localhost\Ubuntu-22.04\home\zhz è¿™æ ·çš„è·¯å¾„ï¼Œosæ¨¡å—é€šå¸¸èƒ½æ­£ç¡®å¤„ç†
        processed_path = os.path.normpath(os.path.abspath(input_path))
    else:
        processed_path = script_dir

    generate_project_info(processed_path)


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\run_bot.py ---

# run_bot.py (V15 - å†³ç­–é“¾æ¡è¯Šæ–­ & æœ€ç»ˆç¨³å®šç‰ˆ)
import sys
from pathlib import Path
import cv2
import mss
import time
import numpy as np
import onnxruntime as ort

project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.perception.detector import AdvancedDetector
from src.world_modeling.state import GameState
from src.world_modeling.world_model import UKFWorldModel
from src.utils.screen_manager import ScreenManager
from src.controls.agent import GameAgent
from src.state_builder import build_state_vector

def main():
    print("ğŸš€ å¯åŠ¨ Dino AI (ä¸“å®¶å¤§è„‘ - æœ€ç»ˆå†³æˆ˜ç‰ˆ)...")

    detector = None
    prev_time = time.time() 

    try:
        detector = AdvancedDetector(
            yolo_model_path="models/detection/dino_detector.onnx",
            classifier_model_path="models/classification/dino_classifier.pth"
        )
        game_state = GameState()
        world_model = UKFWorldModel()
        agent = GameAgent()
        screen_manager = ScreenManager(mss.mss())
    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šåˆå§‹åŒ–æ ¸å¿ƒæ¨¡å—å¤±è´¥: {e}")
        return

    # --- PyTorch Profiler å¯ç”¨ (è¯Šæ–­ç»“æŸåè¯·æ³¨é‡Šæˆ–ç§»é™¤) ---
    # detector.enable_profiler(log_dir="runs/classifier_profiler_logs")
    # ----------------------------------------------------

    onnx_model_path = "models/policy/dino_policy.onnx"
    print(f"ğŸ§  æ­£åœ¨åŠ è½½ä¸“å®¶å¤§è„‘: {onnx_model_path}")
    try:
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        trt_provider_options = {
            "trt_fp16_enable": True,
            "trt_cuda_graph_enable": False, # æš‚æ—¶ç¦ç”¨ CUDA Graphï¼Œå› ä¸ºæœªå®ç° I/O Binding
            "trt_engine_cache_enable": True,
            "trt_engine_cache_path": str(project_root / "models" / "onnx_cache"),
            "trt_max_workspace_size": 2147483648, # 2GB æ˜¾å­˜å·¥ä½œåŒº
        }
        
        providers = [
            ("TensorrtExecutionProvider", trt_provider_options),
            "CUDAExecutionProvider",
            "CPUExecutionProvider",
        ]

        ort_session = ort.InferenceSession(
            onnx_model_path,
            sess_options=session_options,
            providers=providers
        )
        
        (project_root / "models" / "onnx_cache").mkdir(parents=True, exist_ok=True)

        print(f"âœ… ä¸“å®¶å¤§è„‘åŠ è½½æˆåŠŸï¼ä½¿ç”¨è®¾å¤‡: {ort_session.get_providers()}")
        print("æ³¨æ„ï¼šç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼ŒTensorRTæ­£åœ¨æ„å»ºä¼˜åŒ–å¼•æ“å¹¶ç¼“å­˜ã€‚")

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šåŠ è½½ONNXæ¨¡å‹å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ONNX Runtime GPUæ˜¯å¦æ­£ç¡®å®‰è£…ï¼Œä»¥åŠTensorRTç›¸å…³åº“æ˜¯å¦å¯ç”¨ã€‚")
        print("é”™è¯¯è¯¦æƒ…:", e)
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler()
        return
    
    screen_manager.select_roi()
    if screen_manager.roi is None:
        print("ğŸ”´ æœªé€‰æ‹©æ¸¸æˆåŒºåŸŸï¼Œç¨‹åºé€€å‡ºã€‚")
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler() 
        return

    print("3ç§’åæœºå™¨äººå°†å¼€å§‹è¿è¡Œ...")
    time.sleep(3)
    
    try:
        while True:
            frame_start_time = time.perf_counter()

            current_time = time.time()
            dt = current_time - prev_time
            if dt == 0: dt = 1/60
            prev_time = current_time 

            capture_start = time.perf_counter()
            img = screen_manager.capture()
            capture_end = time.perf_counter()
            if img is None: break
            
            detection_start = time.perf_counter()
            detections = detector.detect(img, yolo_class_names=['bird', 'cactus', 'dino'])
            detection_end = time.perf_counter()
            
            game_state_update_start = time.perf_counter()
            game_state.update(detections, dt)
            game_state_update_end = time.perf_counter()

            world_model_update_start = time.perf_counter()
            closest_obs = min(game_state.obstacles, key=lambda o: o[0][0]) if game_state.obstacles else None
            world_model.update(closest_obs, dt)
            world_model_update_end = time.perf_counter()
            
            state_build_start = time.perf_counter()
            state_vector = build_state_vector(game_state, world_model)
            state_build_end = time.perf_counter()

            # --- è¯Šæ–­ GameState, WorldModel, StateVector çš„å†…å®¹ ---
            print(f"DEBUG_DECISION: Dino Box: {game_state.dino_box}")
            print(f"DEBUG_DECISION: Obstacles Count: {len(game_state.obstacles)} | Closest: {closest_obs}")
            pos, speed = world_model.get_state()
            print(f"DEBUG_DECISION: World Model State (Pos, Speed): ({pos:.2f}, {speed:.2f})" if pos is not None else "DEBUG_DECISION: World Model State: None")
            print(f"DEBUG_DECISION: State Vector: {state_vector.round(3) if state_vector is not None else 'None'}")
            
            action_index = 0
            inference_start = time.perf_counter()
            if state_vector is not None:
                input_state_tensor = np.expand_dims(state_vector, axis=0).astype(np.float32)

                input_name = ort_session.get_inputs()[0].name
                output_name = ort_session.get_outputs()[0].name 

                raw_action_result = [] 
                try:
                    raw_action_result = ort_session.run([output_name], {input_name: input_state_tensor})
                except Exception as e:
                    print(f"âŒ ERROR: å†³ç­–æ¨¡å‹ (dino_policy.onnx) ORT run å¤±è´¥ï¼è¯¦ç»†é”™è¯¯: {e}")
                    print(f"  è¾“å…¥æ•°æ®å½¢çŠ¶: {input_state_tensor.shape}, ç±»å‹: {input_state_tensor.dtype}")
                    raw_action_result = [] 

                if raw_action_result and len(raw_action_result) > 0:
                    action_output_tensor = raw_action_result[0]
                    if isinstance(action_output_tensor, np.ndarray) and action_output_tensor.size > 0:
                        action_index = int(action_output_tensor.flatten()[0]) 
                        if action_index not in [0, 1, 2]:
                            print(f"âš ï¸ è­¦å‘Šï¼šå†³ç­–æ¨¡å‹è¿”å›äº†è¶…å‡ºèŒƒå›´çš„åŠ¨ä½œç´¢å¼•: {action_index}ã€‚é»˜è®¤æ‰§è¡Œ 'æ— æ“ä½œ'ã€‚")
                            action_index = 0
                    else:
                        print(f"âš ï¸ è­¦å‘Šï¼šå†³ç­–æ¨¡å‹è¿”å›é numpy æ•°ç»„æˆ–ç©ºæ•°ç»„ã€‚åŸå§‹ç»“æœ: {raw_action_result}ã€‚é»˜è®¤æ‰§è¡Œ 'æ— æ“ä½œ'ã€‚")
                        action_index = 0
                else:
                    print(f"âš ï¸ è­¦å‘Šï¼šå†³ç­–æ¨¡å‹æœªè¿”å›ä»»ä½•ç»“æœã€‚åŸå§‹ç»“æœ: {raw_action_result}ã€‚é»˜è®¤æ‰§è¡Œ 'æ— æ“ä½œ'ã€‚")
                    action_index = 0

            inference_end = time.perf_counter()

            # --- è¯Šæ–­å†³ç­–åŠ¨ä½œå’Œæ‰§è¡Œæƒ…å†µ ---
            print(f"DEBUG_DECISION: Chosen Action Index: {action_index}")

            action_execute_start = time.perf_counter()
            if action_index == 1:
                agent.jump(duration=0.05)
            elif action_index == 2:
                agent.duck()
            action_execute_end = time.perf_counter()

            frame_end_time = time.perf_counter()
            
            print(f"Frame Time: {((frame_end_time - frame_start_time)*1000):.2f}ms | "
                  f"Capture: {((capture_end - capture_start)*1000):.2f}ms | "
                  f"Detect: {((detection_end - detection_start)*1000):.2f}ms | "
                  f"GameState: {((game_state_update_end - game_state_update_start)*1000):.2f}ms | "
                  f"WorldModel: {((world_model_update_end - world_model_update_start)*1000):.2f}ms | "
                  f"StateBuild: {((state_build_end - state_build_start)*1000):.2f}ms | "
                  f"Inference: {((inference_end - inference_start)*1000):.2f}ms | "
                  f"ActionExecute: {((action_execute_end - action_execute_start)*1000):.2f}ms")

            debug_img = img.copy()
            for box, class_name in detections:
                x1, y1, x2, y2 = box
                color_map = {"dino": (0, 255, 0), "cactus": (0, 0, 255), "bird": (255, 0, 0)}
                color = next((c for k, c in color_map.items() if k in class_name), (0, 255, 255))
                cv2.rectangle(debug_img, (x1, y1), (x2, y2), color, 2)
                cv2.putText(debug_img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            _, speed = world_model.get_state()
            speed_text = f"Speed: {speed or 0:.0f}"
            cv2.putText(debug_img, speed_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
            cv2.imshow("Dino AI - Expert Brain (Debug View)", debug_img)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    finally:
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler() 
            
    cv2.destroyAllWindows()
    print("ğŸ¤– Dino AI å·²åœæ­¢è¿è¡Œã€‚")
    
if __name__ == "__main__":
    main()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\__init__.py ---


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\brain.py ---

# src/brain.py
import torch.nn as nn

class ImitationBrain(nn.Module):
    def __init__(self, input_size, output_size):
        super(ImitationBrain, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_size)
        )
    def forward(self, x):
        return self.network(x)
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\config.py ---

# src/config.py

import torch
import onnxruntime

# --- åŠ¨æ€ç¡¬ä»¶æ£€æµ‹ä¸é…ç½® ---
def get_optimal_execution_providers():
    """
    æ™ºèƒ½æ£€æµ‹å¯ç”¨çš„ONNX Runtimeæ‰§è¡Œæä¾›è€…ã€‚
    ä¼˜å…ˆä½¿ç”¨CUDAï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°CPUã€‚
    """
    available_providers = onnxruntime.get_available_providers()
    if 'CUDAExecutionProvider' in available_providers:
        print("Hardware Check: GPU (CUDAExecutionProvider) is available. Using GPU for inference.")
        return ['CUDAExecutionProvider', 'CPUExecutionProvider']
    else:
        print("Hardware Check: GPU (CUDA) not available. Falling back to CPUExecutionProvider.")
        return ['CPUExecutionProvider']

# --- æ„ŸçŸ¥å±‚é…ç½® ---
MODEL_PATH = "models/dino_best.onnx"
# åŠ¨æ€è·å–æœ€ä½³çš„æ‰§è¡Œé…ç½®
ORT_EXECUTION_PROVIDERS = get_optimal_execution_providers() 
# ç½®ä¿¡åº¦é˜ˆå€¼
CONFIDENCE_THRESHOLD = 0.6

# --- ä¸–ç•Œå»ºæ¨¡å±‚é…ç½® ---
GAME_ROI = (0, 0, 800, 600) 

# --- å†³ç­–è§„åˆ’å±‚é…ç½® ---
JUMP_TRIGGER_DISTANCE = 150

# --- åŠ¨ä½œæ‰§è¡Œå±‚é…ç½® ---
ACTION_COOLDOWN = 0.1
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\contracts.py ---

# src/contracts.py
from dataclasses import dataclass, field
from typing import List, Tuple
from enum import Enum

# åŠ¨ä½œå¥‘çº¦ (Action Contract)
class Action(Enum):
    """å®šä¹‰äº†æ‰€æœ‰å¯èƒ½çš„AIåŠ¨ä½œï¼Œç”¨äºå†³ç­–å±‚ä¸æ‰§è¡Œå±‚çš„è§£è€¦ã€‚"""
    NOOP = 0  # No-Operation, æ— æ“ä½œ
    JUMP = 1
    DUCK = 2

# æ„ŸçŸ¥å±‚å¥‘çº¦ (Perception Contract)
@dataclass(frozen=True)
class Detection:
    """å®šä¹‰äº†å•ä¸ªæ£€æµ‹åˆ°çš„ç‰©ä½“ï¼Œç”±æ„ŸçŸ¥å±‚è¾“å‡ºã€‚"""
    label: str
    box: Tuple[int, int, int, int] # (x1, y1, x2, y2)
    confidence: float

# ä¸–ç•Œå»ºæ¨¡å±‚å¥‘çº¦ (World Modeling Contract)
@dataclass(frozen=True)
class Obstacle:
    """å®šä¹‰äº†å•ä¸ªéšœç¢ç‰©çš„å®Œæ•´çŠ¶æ€ï¼Œç”±ä¸–ç•Œæ¨¡å‹è®¡ç®—ã€‚"""
    label: str
    box: Tuple[int, int, int, int]
    distance: float # ä¸æé¾™çš„æ°´å¹³è·ç¦»
    speed: float    # ä¼°ç®—å‡ºçš„æ°´å¹³é€Ÿåº¦

@dataclass(frozen=True)
class WorldState:
    """å®šä¹‰äº†æŸä¸€å¸§çš„å®Œæ•´ä¸–ç•ŒçŠ¶æ€ï¼Œæ˜¯ä¸–ç•Œæ¨¡å‹çš„æ ¸å¿ƒè¾“å‡ºã€‚"""
    is_valid: bool # çŠ¶æ€æ˜¯å¦æœ‰æ•ˆï¼ˆä¾‹å¦‚ï¼Œæ˜¯å¦æ£€æµ‹åˆ°æé¾™ï¼‰
    dino_box: Tuple[int, int, int, int]
    # æ€»æ˜¯è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå³ä½¿æ²¡æœ‰éšœç¢ç‰©
    obstacles: List[Obstacle] = field(default_factory=list)

    @property
    def closest_obstacle(self) -> Obstacle | None:
        """æä¾›ä¸€ä¸ªæ–¹ä¾¿çš„æ¥å£æ¥è·å–æœ€è¿‘çš„éšœç¢ç‰©ã€‚"""
        if not self.obstacles:
            return None
        return min(self.obstacles, key=lambda o: o.distance)
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\controls\__init__.py ---


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\controls\agent.py ---

# src/controls/agent.py (V4 - çŠ¶æ€æ„ŸçŸ¥ç‰ˆ & å¢å¼ºæ³¨é‡Š)
import time
from pynput.keyboard import Controller, Key

class GameAgent:
    """
    åŠ¨ä½œæ‰§è¡Œå±‚ï¼šè´Ÿè´£å°†å†³ç­–è§„åˆ’å±‚è¾“å‡ºçš„æŠ½è±¡åŠ¨ä½œï¼ˆå¦‚â€œè·³è·ƒâ€ã€â€œä¸‹è¹²â€ï¼‰ï¼Œ
    è½¬åŒ–ä¸ºå¯¹æ¸¸æˆçª—å£çš„å…·ä½“é”®ç›˜è¾“å…¥ã€‚
    """
    def __init__(self):
        # last_action_time: è®°å½•ä¸Šä¸€æ¬¡åŠ¨ä½œçš„æ—¶é—´æˆ³
        self.last_action_time = 0
        
        # is_busy å’Œ busy_until_time: ç”¨äºå®ç°ä¸€ä¸ªç®€å•çš„åŠ¨ä½œå†·å´æˆ–çŠ¶æ€é”å®šã€‚
        # åœ¨L2é˜¶æ®µï¼Œè¿™ç”¨äºé˜²æ­¢è¡Œä¸ºæ ‘åœ¨ä¸€ä¸ªåŠ¨ä½œæœªå®Œæˆæ—¶è§¦å‘ä¸‹ä¸€ä¸ªã€‚
        # åœ¨L3é˜¶æ®µï¼Œè¿™å¯ä»¥ä½œä¸ºä¸€ä¸ªå¯é€‰çš„ä¿æŠ¤æœºåˆ¶ï¼Œé˜²æ­¢å†³ç­–æ¨¡å‹è¿‡äºâ€œæŠ½æâ€ã€‚
        # å½“å‰åœ¨ run_bot.py ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰ä¸»åŠ¨è°ƒç”¨ check_busy()ï¼Œ
        # æ‰€ä»¥è¿™ä¸ªæœºåˆ¶ç›®å‰æ˜¯â€œä¼‘çœ â€çš„ã€‚
        self.is_busy = False 
        self.busy_until_time = 0 
        
        # åˆå§‹åŒ– pynput çš„é”®ç›˜æ§åˆ¶å™¨
        self.keyboard = Controller()

    def _update_busy_state(self, duration):
        """
        å†…éƒ¨æ–¹æ³•ï¼šæ›´æ–°agentçš„å¿™ç¢ŒçŠ¶æ€å’Œæ—¶é—´ã€‚
        å½“ä¸€ä¸ªåŠ¨ä½œè¢«æ‰§è¡Œæ—¶ï¼Œå°†agentæ ‡è®°ä¸ºâ€œå¿™ç¢Œâ€ï¼Œç›´åˆ°æŒ‡å®šçš„æŒç»­æ—¶é—´ç»“æŸã€‚
        """
        self.is_busy = True
        self.last_action_time = time.time()
        self.busy_until_time = self.last_action_time + duration

    def check_busy(self):
        """
        å¤–éƒ¨æ£€æŸ¥agentæ˜¯å¦å¿™ç¢Œã€‚å¦‚æœå¿™ç¢Œæ—¶é—´å·²è¿‡ï¼Œåˆ™é‡ç½®çŠ¶æ€ã€‚
        """
        if self.is_busy and time.time() > self.busy_until_time:
            self.is_busy = False 
        return self.is_busy

    def jump(self, duration=0.05):
        """
        æ‰§è¡Œâ€œè·³è·ƒâ€åŠ¨ä½œã€‚
        æ¨¡æ‹ŸæŒ‰ä¸‹å¹¶é‡Šæ”¾ç©ºæ ¼é”®ã€‚

        Args:
            duration (float): æŒ‰ä¸‹ç©ºæ ¼é”®çš„æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
        """
        # è·³è·ƒåŠ¨ä½œçš„æ€»æŒç»­æ—¶é—´ï¼Œå¯ä»¥ç•¥é•¿äºæŒ‰é”®æ—¶é—´ï¼Œç»™ä¸€ä¸ªè½åœ°ç¼“å†²ã€‚
        # è¿™ä¸»è¦ç”¨äº _update_busy_stateï¼Œä»¥å®šä¹‰ä¸€ä¸ªåˆç†çš„â€œå†·å´æ—¶é—´â€ã€‚
        JUMP_TOTAL_DURATION = 0.45 
        self._update_busy_state(JUMP_TOTAL_DURATION)
        
        print(f"[Action] JUMP triggered.") # å¢åŠ è¯Šæ–­æ‰“å°
        self.keyboard.press(Key.space)
        time.sleep(duration) # æŒ‰ä½ç©ºæ ¼é”®ä¸€å°æ®µæ—¶é—´
        self.keyboard.release(Key.space)
        print(f"[Action] JUMP completed (busy for {JUMP_TOTAL_DURATION}s).")

    def duck(self):
        """
        æ‰§è¡Œâ€œä¸‹è¹²â€åŠ¨ä½œã€‚
        æ¨¡æ‹ŸæŒ‰ä¸‹å¹¶é‡Šæ”¾å‘ä¸‹ç®­å¤´é”®ã€‚
        """
        DUCK_TOTAL_DURATION = 0.4
        self._update_busy_state(DUCK_TOTAL_DURATION)

        print(f"[Action] DUCK triggered.") # å¢åŠ è¯Šæ–­æ‰“å°
        self.keyboard.press(Key.down)
        time.sleep(0.3) # æŒ‰ä½ä¸‹ç®­å¤´é”®
        self.keyboard.release(Key.down)
        print(f"[Action] DUCK completed (busy for {DUCK_TOTAL_DURATION}s).")
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\perception\__init__.py ---


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\perception\detector.py ---

# src/perception/detector.py (V16 - é¢„å¤„ç†ä¸åå¤„ç†é»„é‡‘æ ‡å‡†ç‰ˆ)

import torch
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image
from torch.profiler import profile, schedule, tensorboard_trace_handler
import onnxruntime as ort 
from pathlib import Path 

# --- å¯¼å…¥ Ultralytics å®˜æ–¹è¾…åŠ©å‡½æ•° ---
from ultralytics.utils.ops import non_max_suppression, scale_boxes

# --- è¾…åŠ©å‡½æ•°ï¼šLetterbox é¢„å¤„ç† (æ¥è‡ªç ”ç©¶æŠ¥å‘Šï¼Œç²¾ç¡®å¤ç°) ---
def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):
    # Resize and pad image while meeting stride-multiple constraints
    shape = im.shape[:2]  # current shape [height, width] (H, W)
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape) # new_shape is (W, H)

    # Scale ratio (new / old)
    # new_shape[0] is target width, shape[1] is current width
    # new_shape[1] is target height, shape[0] is current height
    r = min(new_shape[0] / shape[1], new_shape[1] / shape[0])
    if not scaleup:  # only scale down, do not scale up (for better val mAP)
        r = min(r, 1.0)

    # Compute padding
    # new_unpad is (width, height)
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))

    # dw, dh are padding amounts for width and height
    # new_shape[0] is target width, new_unpad[0] is current unpadded width
    dw, dh = new_shape[0] - new_unpad[0], new_shape[1] - new_unpad[1]  
    
    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
    elif not scaleup: # Added from official source, ensures padding is non-negative if not scaling up
        dw, dh = max(0, dw), max(0, dh)

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize (shape[::-1] is (width, height))
        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR) # new_unpad is (width, height)
    
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    return im, r, (dw, dh) # Return img, ratio (single float r), and pad (tuple (dw, dh))

# --- è¾…åŠ©å‡½æ•°ï¼šå›¾åƒé¢„å¤„ç† (æ•´åˆ Letterbox, RGBè½¬æ¢ï¼Œå½’ä¸€åŒ–ï¼ŒCHWè½¬æ¢) (æ¥è‡ªç ”ç©¶æŠ¥å‘Š) ---
def preprocess_image_for_yolo(original_image, new_shape=(640, 640)):
    # original_image is expected to be BGR from OpenCV imread
    # 1. BGR åˆ° RGB è½¬æ¢ (å¦‚æœä½¿ç”¨ OpenCV è¯»å–å›¾åƒ)
    img_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

    # 2. åº”ç”¨ letterbox è°ƒæ•´å¤§å°å’Œå¡«å……
    # new_shape=(width, height) for letterbox
    img_padded, ratio, pad = letterbox(img_rgb, new_shape=new_shape, auto=False, scaleup=True) 
    
    # 3. é€šé“è½¬ç½® (HWC åˆ° CHW) (é«˜, å®½, é€šé“ -> é€šé“, é«˜, å®½)
    img_chw = img_padded.transpose((2, 0, 1))

    # 4. å½’ä¸€åŒ–åƒç´ å€¼ä» 0-255 åˆ° 0-1
    img_normalized = np.ascontiguousarray(img_chw, dtype=np.float32) # Ensure contiguous memory
    img_normalized /= 255.0  # Normalize to 0-1

    # 5. æ·»åŠ æ‰¹æ¬¡ç»´åº¦ (1, C, H, W)
    img_final = np.expand_dims(img_normalized, 0)

    return img_final, ratio, pad # Return processed image, scale ratio, and padding

class AdvancedDetector:
    def __init__(self, yolo_model_path, classifier_model_path):
        try:
            if torch.cuda.is_available():
                self.device = 'cuda'
                print("âœ… Hardware Check: GPU (CUDA) is available. Inference will run on GPU.")
            else:
                self.device = 'cpu'
                print("âš ï¸ Hardware Check: GPU (CUDA) not found. Inference will fall back to CPU.")

            print(f"ğŸ§  æ­£åœ¨åŠ è½½ YOLO Detector æ¨¡å‹: {yolo_model_path}")
            
            yolo_session_options = ort.SessionOptions()
            yolo_session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

            yolo_trt_provider_options = {
                "trt_fp16_enable": True,
                "trt_cuda_graph_enable": False, # æš‚æ—¶ç¦ç”¨ CUDA Graphï¼Œå› ä¸ºæœªå®ç° I/O Binding
                "trt_engine_cache_enable": True,
                "trt_engine_cache_path": str(Path(yolo_model_path).parent / "onnx_cache"),
                "trt_max_workspace_size": 1073741824, # 1GB æ˜¾å­˜å·¥ä½œåŒº
            }

            yolo_providers = [
                ("TensorrtExecutionProvider", yolo_trt_provider_options),
                "CUDAExecutionProvider",
                "CPUExecutionProvider",
            ]
            
            self.yolo_ort_session = ort.InferenceSession(
                yolo_model_path,
                sess_options=yolo_session_options,
                providers=yolo_providers
            )
            (Path(yolo_model_path).parent / "onnx_cache").mkdir(parents=True, exist_ok=True)
            print(f"âœ… YOLO Detector ONNX Runtime Session åŠ è½½æˆåŠŸã€‚ä½¿ç”¨è®¾å¤‡: {self.yolo_ort_session.get_providers()}")
            print("æ³¨æ„ï¼šYOLO Detectorç¬¬ä¸€æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢ï¼ŒTensorRTæ­£åœ¨æ„å»ºä¼˜åŒ–å¼•æ“å¹¶ç¼“å­˜ã€‚")
            
        except Exception as e:
            print(f"âŒ YOLO Detector: æ¨¡å‹åŠ è½½å¤±è´¥ã€‚é”™è¯¯è¯¦æƒ…: {e}") 
            raise

        try:
            checkpoint = torch.load(classifier_model_path, map_location=self.device)
            self.class_names_classifier = checkpoint['class_names']
            num_classes = len(self.class_names_classifier)
            
            self.classifier_model = models.efficientnet_v2_s()
            num_ftrs = self.classifier_model.classifier[1].in_features
            self.classifier_model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)
            
            self.classifier_model.load_state_dict(checkpoint['model_state_dict'])
            self.classifier_model = self.classifier_model.to(self.device)
            self.classifier_model.eval()
            print(f"âœ… Classifier: æ¨¡å‹ '{classifier_model_path}' åŠ è½½æˆåŠŸã€‚")
            print(f"   -> åˆ†ç±»å™¨ç±»åˆ«: {self.class_names_classifier}")
            
            self.target_size = 224
            # --- [æ ¸å¿ƒä¿®æ”¹ 1] ä¿®æ­£ classifier_transform ä¸­ letterbox çš„å¼•ç”¨ ---
            # letterbox ç°åœ¨æ˜¯æ–‡ä»¶é¡¶éƒ¨çš„ç‹¬ç«‹å‡½æ•°ï¼Œè€Œä¸æ˜¯ç±»æ–¹æ³•
            self.classifier_transform = transforms.Compose([
                # ç›´æ¥è°ƒç”¨æ–‡ä»¶é¡¶éƒ¨çš„ letterbox å‡½æ•°ï¼Œè€Œä¸æ˜¯ self.letterbox
                transforms.Lambda(lambda img: letterbox(img, new_shape=(self.target_size, self.target_size), auto=False, scaleup=False)[0]), # letterboxè¿”å›(img, ratio, pad)ï¼Œæˆ‘ä»¬éœ€è¦img
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
            # --------------------------------------------------------------------
        except Exception as e:
            print(f"âŒ Classifier: æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
            
        self.last_obstacle_rois = []
        self.profiler = None

    def enable_profiler(self, log_dir="runs/profiler_logs"):
        """åœ¨éœ€è¦æ—¶æ‰‹åŠ¨è°ƒç”¨ä»¥å¯ç”¨profiler"""
        if self.profiler is None:
            sch = schedule(wait=20, warmup=10, active=10, repeat=1)
            self.profiler = profile(
                schedule=sch,
                on_trace_ready=tensorboard_trace_handler(log_dir),
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                record_shapes=True,
                profile_memory=True,
                with_stack=True 
            )
            self.profiler.start()
            print(f"PyTorch Profiler started, logs will be saved to {log_dir}")

    def disable_profiler(self):
        """åœ¨è¯Šæ–­ç»“æŸåæ‰‹åŠ¨è°ƒç”¨ä»¥ç¦ç”¨profiler"""
        if self.profiler is not None:
            self.profiler.stop()
            print(self.profiler.key_averages().table(sort_by="cuda_time_total", row_limit=10)) 
            print("PyTorch Profiler stopped.")
            self.profiler = None
            
    def _postprocess_yolo8_output(self, output, original_img_shape, ratio, pad, 
                                  conf_thres=0.01, iou_thres=0.45, yolo_class_names=None): # é»˜è®¤ç½®ä¿¡åº¦æ”¹ä¸º 0.01
        """
        åå¤„ç†YOLOv8 ONNX (1,7,8400) åŸå§‹è¾“å‡º
        output: (1,7,8400) ONNXåŸå§‹è¾“å‡º (NumPy array)
        original_img_shape: (height, width) åŸå§‹å›¾åƒå°ºå¯¸
        ratio: Letterbox é¢„å¤„ç†ä¸­çš„ç¼©æ”¾æ¯”ä¾‹
        pad: (dw, dh) Letterbox é¢„å¤„ç†ä¸­çš„å¡«å……é‡
        """
        if yolo_class_names is None:
            yolo_class_names = ['bird', 'cactus', 'dino'] 

        # 1. è½¬ç½®å¹¶ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼šä» (1, 7, 8400) åˆ° (8400, 7)
        predictions = np.squeeze(output).T 

        # 2. åˆ†ç¦»ç»„ä»¶å’Œè®¡ç®—ç½®ä¿¡åº¦ (æŠ¥å‘ŠæŒ‡å‡ºï¼šYOLOv8è¾“å‡ºå·²åŒ…å«æœ€ç»ˆç½®ä¿¡åº¦ï¼Œæ— éœ€é¢å¤–obj_conf * class_score)
        boxes_xywh_raw = predictions[:, :4]  # x, y, w, h (å½’ä¸€åŒ–ï¼Œç›¸å¯¹ 640x640 letterboxå›¾åƒ)
        class_scores_raw = predictions[:, 4:] # ç±»åˆ«åˆ†æ•° (report says 4th index onwards are class confs)

        # æ‰¾åˆ°æœ€é«˜åˆ†æ•°çš„ç±»åˆ«IDå’Œå…¶ç½®ä¿¡åº¦
        max_class_scores = np.max(class_scores_raw, axis=1) # è¿™å°±æ˜¯æœ€ç»ˆç½®ä¿¡åº¦
        class_ids_raw = np.argmax(class_scores_raw, axis=1)

        raw_scores_sorted_idx = np.argsort(max_class_scores)[::-1] 
        top_k_print = min(10, len(raw_scores_sorted_idx)) 
        
        if top_k_print > 0:
            top_k_conf_scores = max_class_scores[raw_scores_sorted_idx[:top_k_print]]
            top_k_class_ids = class_ids_raw[raw_scores_sorted_idx[:top_k_print]]
            
            top_k_class_names = []
            for cid in top_k_class_ids:
                if cid >= 0 and cid < len(yolo_class_names): 
                    top_k_class_names.append(yolo_class_names[cid])
                else:
                    top_k_class_names.append(f"unknown_cls_{cid}")

            print(f"DEBUG: Top-{top_k_print} Raw Pred Confidences: {top_k_conf_scores.round(3)}, Class_IDs: {top_k_class_ids}, Names: {top_k_class_names}")
            if np.max(top_k_conf_scores) < conf_thres:
                print(f"DEBUG: æœ€é«˜åŸå§‹ç½®ä¿¡åº¦ ({np.max(top_k_conf_scores):.3f}) ä½äºé˜ˆå€¼ ({conf_thres:.3f})ï¼Œå¯èƒ½æ— æ£€æµ‹ç»“æœã€‚")
        else:
            print("DEBUG: æ— åŸå§‹é¢„æµ‹æ¡†ã€‚")

        # 5. æ ¹æ®ç½®ä¿¡åº¦é˜ˆå€¼è¿›è¡Œç­›é€‰
        keep_indices_conf = max_class_scores > conf_thres
        boxes_xywh_filtered = boxes_xywh_raw[keep_indices_conf]
        max_class_scores_filtered = max_class_scores[keep_indices_conf]
        class_ids_filtered = class_ids_raw[keep_indices_conf]

        if len(boxes_xywh_filtered) == 0:
            print(f"DEBUG: ç»ç½®ä¿¡åº¦ {conf_thres} ç­›é€‰åï¼Œæ— æœ‰æ•ˆæ£€æµ‹æ¡†ã€‚") 
            return [], [], []

        # 6. åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶ (NMS) - ä½¿ç”¨ Ultralytics å®˜æ–¹ NMS
        # å°† NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch Tensor for NMS
        boxes_xyxy_tensor = torch.from_numpy(boxes_xywh_filtered).float() # (M, 4) in xywh format
        scores_tensor = torch.from_numpy(max_class_scores_filtered).float() # (M,)

        # Convert xywh to xyxy for NMS (torchvision.ops.nms or ultralytics.utils.ops.non_max_suppression expects xyxy)
        # Note: ultralytics non_max_suppression expects (batch_size, num_detections, 6) or (batch_size, num_detections, 4+num_classes)
        # The structure is [x1, y1, x2, y2, conf, class_id] after non_max_suppression.
        # So we need to format it before passing to non_max_suppression.
        # Let's rebuild the input to non_max_suppression to match its expectation.

        # Predictions for NMS need to be (N, 6) or (N, 4+num_classes) for NMS
        # Let's make it (N, 6) -> [x1, y1, x2, y2, conf, class_id]
        
        # Convert xywh to xyxy (normalized to 640x640 space) for NMS
        # This is where non_max_suppression expects its boxes
        
        # We need to construct a tensor of shape (N, 6) from our filtered data
        # [x, y, w, h, max_class_score, class_id]
        
        # Convert xywh to xyxy
        x1 = boxes_xywh_filtered[:, 0] - boxes_xywh_filtered[:, 2] / 2
        y1 = boxes_xywh_filtered[:, 1] - boxes_xywh_filtered[:, 3] / 2
        x2 = boxes_xywh_filtered[:, 0] + boxes_xywh_filtered[:, 2] / 2
        y2 = boxes_xywh_filtered[:, 1] + boxes_xywh_filtered[:, 3] / 2

        # Combine into (N, 6) tensor for NMS
        # (x1, y1, x2, y2, conf, class_id)
        nms_input = np.concatenate((x1[:, np.newaxis], y1[:, np.newaxis], 
                                    x2[:, np.newaxis], y2[:, np.newaxis], 
                                    max_class_scores_filtered[:, np.newaxis], 
                                    class_ids_filtered[:, np.newaxis]), axis=1)
        
        nms_input_tensor = torch.from_numpy(nms_input).float()
        
        # NMS è¿”å›ä¸€ä¸ªå¼ é‡åˆ—è¡¨ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯å¼ å›¾åƒå¯¹åº”ä¸€ä¸ªå¼ é‡ã€‚
        # æ¯ä¸ªå¼ é‡åŒ…å«æ£€æµ‹åˆ°çš„å¯¹è±¡: [x1, y1, x2, y2, conf, class_id]
        results_from_nms = non_max_suppression(
            nms_input_tensor.unsqueeze(0), # Add batch dimension for NMS input (1, N, 6)
            conf_thres=conf_thres,         # ä½¿ç”¨ä¼ å…¥çš„ conf_thres
            iou_thres=iou_thres,           # ä½¿ç”¨ä¼ å…¥çš„ iou_thres
            classes=None,                  # ä¸æŒ‰ç‰¹å®šç±»åˆ«è¿‡æ»¤
            agnostic=False,                # ä¸è¿›è¡Œç±»åˆ«æ— å…³NMS
            max_det=1000                   # æ¯å¼ å›¾åƒçš„æœ€å¤§æ£€æµ‹æ•°é‡
        )
        
        if not results_from_nms or len(results_from_nms[0]) == 0: # NMS åæ— ç»“æœ
            print(f"DEBUG: ç» NMS (IoU={iou_thres}, Conf={conf_thres}) åï¼Œæ— æœ‰æ•ˆæ£€æµ‹æ¡†ã€‚") 
            return [], [], []

        # results_from_nms[0] æ˜¯ä¸€ä¸ª PyTorch Tensorï¼Œå½¢çŠ¶ä¸º (num_final_detections, 6)
        final_detections_tensor = results_from_nms[0] 

        # è½¬æ¢ä¸º NumPy æ•°ç»„
        final_detections_np = final_detections_tensor.cpu().numpy()

        # æå– NMS åçš„æœ€ç»ˆæ¡†ã€ç½®ä¿¡åº¦å’Œç±»åˆ«ID
        final_boxes_xyxy_640 = final_detections_np[:, :4] # æ¡†åœ¨ 640x640 letterbox ç©ºé—´
        final_scores_after_nms = final_detections_np[:, 4]
        final_class_ids_after_nms = final_detections_np[:, 5].astype(int)

        # 8. ç²¾ç¡®åæ ‡åå˜æ¢ï¼šä» 640x640 letterbox å›¾åƒæ˜ å°„å›åŸå§‹å›¾åƒåƒç´ 
        # ä½¿ç”¨ Ultralytics å®˜æ–¹çš„ scale_boxes å‡½æ•°
        # original_img_shape is (H, W)
        # new_shape is (W, H)
        # scale_boxes expects (img1_shape, boxes, img0_shape) for scaling boxes from img1 to img0
        # img1_shape: (H, W) of the image where boxes are currently defined (640, 640)
        # img0_shape: (H, W) of the target image (original_image_h, original_image_w)

        # Corrected: scale_boxes expects input boxes to be in (x1, y1, x2, y2) format already
        # And expects img1_shape (H,W) and img0_shape (H,W)
        
        # Convert final_boxes_xyxy_640 (W,H) format to (H,W) for scale_boxes
        # (640,640) is the input_shape to the model which is (W,H) in letterbox function, so it matches.
        
        # Scale boxes back to original image size
        # scale_boxes expects (img1_shape, boxes, img0_shape)
        # img1_shape is (H, W) where boxes are currently defined (640, 640)
        # img0_shape is (H, W) of the target image (original_image_h, original_image_w)
        # The boxes are already x1y1x2y2 format in 640x640 space
        
        # Use new_shape as (H,W) for scale_boxes for clarity
        model_input_hw = (640, 640) # YOLOv8 model input H, W (after letterbox)

        final_boxes_original_scaled = scale_boxes(
            model_input_hw, # Source shape (H,W)
            torch.from_numpy(final_boxes_xyxy_640).float(), # Boxes in source shape
            original_img_shape # Target shape (H,W)
        ).numpy()

        # ç¡®ä¿åæ ‡æ˜¯æ•´æ•°
        final_boxes_original_scaled = final_boxes_original_scaled.astype(int)

        final_boxes_processed = final_boxes_original_scaled
        final_scores = final_scores_after_nms
        final_class_ids = final_class_ids_after_nms
        
        return final_boxes_processed, final_scores, final_class_ids


    def detect(self, image, yolo_class_names, confidence_threshold=0.01): # é»˜è®¤ç½®ä¿¡åº¦æ”¹ä¸º 0.01 (ç”¨äºè°ƒè¯•)
        self.last_obstacle_rois = []
        final_detections = []
        obstacle_rois = []
        obstacle_boxes = []

        original_image_h, original_image_w, _ = image.shape 
        
        # --- [æ ¸å¿ƒä¿®æ”¹ 1] ä½¿ç”¨ preprocess_image_for_yolo è¿›è¡Œé¢„å¤„ç† ---
        processed_img_tensor, ratio_scale, pad_amount = preprocess_image_for_yolo(image, new_shape=(640, 640)) # new_shape=(W,H) for letterbox input_size=640
        
        yolo_input_name = self.yolo_ort_session.get_inputs()[0].name
        
        raw_outputs = [] 
        try:
            # æ˜¾å¼è¯·æ±‚è¾“å‡ºåç§° 'output0' (æ ¹æ®è„šæœ¬è¾“å‡º)
            raw_outputs = self.yolo_ort_session.run(['output0'], {yolo_input_name: processed_img_tensor})
        except Exception as e:
            print(f"âŒ ERROR: YOLO ORT session.run failed during inference. Error: {e}")
            return [] 
        
        if not raw_outputs or not isinstance(raw_outputs[0], np.ndarray) or raw_outputs[0].size == 0:
            print(f"âš ï¸ è­¦å‘Šï¼šYOLOæ¨¡å‹åŸå§‹è¾“å‡ºç»“æœä¸ºç©ºæˆ–éæ•°ç»„ã€‚åŸå§‹è¾“å‡º: {raw_outputs}ã€‚è·³è¿‡æ£€æµ‹ã€‚")
            return [] 

        # --- [æ ¸å¿ƒä¿®æ”¹ 2] è°ƒç”¨ _postprocess_yolo8_output è¿›è¡Œåå¤„ç† ---
        # ä¼ å…¥åŸå§‹å›¾åƒå°ºå¯¸ã€letterboxçš„ ratio å’Œ pad
        processed_boxes, processed_scores, processed_class_ids = self._postprocess_yolo8_output(
            raw_outputs[0], # yolo_outputs[0] æ˜¯ (1, 7, 8400) çš„ NumPy æ•°ç»„
            original_img_shape=(original_image_h, original_image_w), 
            ratio=ratio_scale, 
            pad=pad_amount, # pad_amount æ˜¯ (dw, dh)
            conf_thres=confidence_threshold, 
            iou_thres=0.45, 
            yolo_class_names=yolo_class_names
        )
        
        # ä» processed_boxes ä¸­åˆ†ç¦»å‡º dino å’Œéšœç¢ç‰©ï¼Œå¹¶è¿›è¡Œåç»­å¤„ç†
        for i in range(len(processed_boxes)):
            box = processed_boxes[i]
            class_id = processed_class_ids[i]
            
            yolo_name_from_id = yolo_class_names[class_id] if class_id < len(yolo_class_names) else f"unknown_id_{class_id}"
            
            if yolo_name_from_id == 'dino':
                final_detections.append((box, 'dino-player'))
            elif yolo_name_from_id in ['cactus', 'bird']:
                x1, y1, x2, y2 = box
                padding = 10 
                h_img, w_img, _ = image.shape # image æ˜¯åŸå§‹å›¾åƒ
                x1_pad = max(0, x1 - padding)
                y1_pad = max(0, y1 - padding)
                x2_pad = min(w_img, x2 + padding)
                y2_pad = min(h_img, y2 + padding) 

                roi = image[y1_pad:y2_pad, x1_pad:x2_pad]
                
                if roi.size > 0:
                    obstacle_rois.append(roi)
                    obstacle_boxes.append(box) 
        
        if obstacle_rois:
            if self.profiler is not None:
                self.profiler.step()
            
            self.last_obstacle_rois = obstacle_rois
            batch_tensor = torch.stack([self.classifier_transform(roi) for roi in obstacle_rois]).to(self.device)
            
            with torch.no_grad():
                outputs = self.classifier_model(batch_tensor)
                _, preds = torch.max(outputs, 1)
            
            for i, pred_idx in enumerate(preds):
                specific_class_name = self.class_names_classifier[pred_idx.item()]
                final_detections.append((obstacle_boxes[i], specific_class_name))
                
        return final_detections
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\state_builder.py ---

# src/state_builder.py (V2 - é»„é‡‘æ ‡å‡†æ ¡éªŒç‰ˆ & å¢å¼ºæ³¨é‡Š)
import numpy as np

def normalize(value, min_val, max_val):
    """
    å°†ä¸€ä¸ªå€¼å½’ä¸€åŒ–åˆ° [0, 1] åŒºé—´ã€‚
    """
    # é˜²æ­¢é™¤ä»¥é›¶
    if (max_val - min_val) == 0: 
        return 0.0
    # ä½¿ç”¨ clip ç¡®ä¿ç»“æœåœ¨ [0, 1] èŒƒå›´å†…
    return np.clip((value - min_val) / (max_val - min_val), 0.0, 1.0)

def build_state_vector(game_state, world_model):
    """
    æ„å»ºå†³ç­–æ¨¡å‹æ‰€éœ€çš„çŠ¶æ€å‘é‡ã€‚
    è¯¥å‘é‡çš„ç»´åº¦å’Œæ¯ä¸ªç»´åº¦çš„å«ä¹‰ï¼Œå¿…é¡»ä¸è®­ç»ƒå†³ç­–æ¨¡å‹æ—¶å®Œå…¨ä¸€è‡´ã€‚

    å½“å‰çŠ¶æ€å‘é‡ (7ä¸ªç»´åº¦):
    [
        0: æ¸¸æˆé€Ÿåº¦ (å½’ä¸€åŒ–),
        1: æœ€è¿‘éšœç¢ç‰©ä¸æé¾™çš„è·ç¦» (å½’ä¸€åŒ–),
        2: æœ€è¿‘éšœç¢ç‰©çš„é«˜åº¦ (å½’ä¸€åŒ–),
        3: æœ€è¿‘éšœç¢ç‰©çš„å®½åº¦ (å½’ä¸€åŒ–),
        4: ç¬¬äºŒè¿‘éšœç¢ç‰©ä¸æé¾™çš„è·ç¦» (å½’ä¸€åŒ–),
        5: ç¬¬äºŒè¿‘éšœç¢ç‰©çš„é«˜åº¦ (å½’ä¸€åŒ–),
        6: ç¬¬äºŒè¿‘éšœç¢ç‰©çš„å®½åº¦ (å½’ä¸€åŒ–)
    ]
    """
    # --- [æ ¸å¿ƒ] å®šä¹‰å½’ä¸€åŒ–çš„æœ€å¤§å€¼ï¼Œè¿™äº›å€¼åº”ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ ---
    MAX_SPEED = 1000  # æ¸¸æˆæœ€å¤§é€Ÿåº¦çš„ä¼°è®¡å€¼
    MAX_DIST = 800    # éšœç¢ç‰©æœ€å¤§è·ç¦»çš„ä¼°è®¡å€¼
    MAX_HEIGHT = 100  # éšœç¢ç‰©æœ€å¤§é«˜åº¦çš„ä¼°è®¡å€¼
    MAX_WIDTH = 100   # éšœç¢ç‰©æœ€å¤§å®½åº¦çš„ä¼°è®¡å€¼

    # ä» game_state å’Œ world_model ä¸­è·å–åŸå§‹ä¿¡æ¯
    dino_box = game_state.dino_box
    obstacles = game_state.obstacles
    _, speed = world_model.get_state()
    
    # ç¡®ä¿é€Ÿåº¦ä¸ä¸ºNone
    current_speed = speed if speed is not None else 0
    
    # åˆå§‹åŒ–çŠ¶æ€å‘é‡ï¼Œç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ¸¸æˆé€Ÿåº¦
    state = [normalize(current_speed, 0, MAX_SPEED)]
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°æé¾™ï¼Œåˆ™æ— æ³•è®¡ç®—è·ç¦»ï¼Œè¿”å› None è¡¨ç¤ºçŠ¶æ€æ— æ•ˆ
    if dino_box is None:
        return None

    # æŒ‰xåæ ‡å¯¹éšœç¢ç‰©è¿›è¡Œæ’åºï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½æ‰¾åˆ°æœ€è¿‘å’Œç¬¬äºŒè¿‘çš„
    obstacles.sort(key=lambda o: o[0][0])
    
    # å¡«å……æœ€è¿‘çš„ä¸¤ä¸ªéšœç¢ç‰©ä¿¡æ¯
    for i in range(2):
        if i < len(obstacles):
            # å¦‚æœå­˜åœ¨ç¬¬ i ä¸ªéšœç¢ç‰©
            obs_box, _ = obstacles[i]
            
            # è®¡ç®—éšœç¢ç‰©ä¸æé¾™å³ä¾§è¾¹ç¼˜çš„è·ç¦»
            dist = obs_box[0] - dino_box[2]
            # è®¡ç®—éšœç¢ç‰©çš„é«˜åº¦å’Œå®½åº¦
            height = obs_box[3] - obs_box[1]
            width = obs_box[2] - obs_box[0]
            
            # å°†å½’ä¸€åŒ–åçš„ç‰¹å¾æ·»åŠ åˆ°çŠ¶æ€å‘é‡
            state.extend([
                normalize(dist, 0, MAX_DIST), 
                normalize(height, 0, MAX_HEIGHT), 
                normalize(width, 0, MAX_WIDTH)
            ])
        else:
            # å¦‚æœä¸å­˜åœ¨ç¬¬ i ä¸ªéšœç¢ç‰©ï¼ˆä¾‹å¦‚ï¼Œåªæœ‰ä¸€ä¸ªæˆ–æ²¡æœ‰éšœç¢ç‰©ï¼‰
            # ä½¿ç”¨é»˜è®¤å€¼å¡«å……ï¼Œè¡¨ç¤ºâ€œæ²¡æœ‰éšœç¢ç‰©â€
            # [1.0, 0.0, 0.0] è¡¨ç¤ºï¼šè·ç¦»æ— ç©·å¤§ (å½’ä¸€åŒ–ä¸º1), é«˜åº¦ä¸º0, å®½åº¦ä¸º0
            state.extend([1.0, 0.0, 0.0])
            
    # å°† state åˆ—è¡¨è½¬æ¢ä¸º float32 ç±»å‹çš„ NumPy æ•°ç»„
    return np.array(state, dtype=np.float32)
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\utils\__init__.py ---


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\utils\screen_manager.py ---

# src/utils/screen_manager.py (V4 - å½»åº•åˆ†ç¦»ç‰ˆ)
import cv2
import mss
import numpy as np
from PIL import ImageGrab
import os # ç”¨äºæ–‡ä»¶æ“ä½œ

class ScreenManager:
    def __init__(self, sct_instance):
        self.roi = None
        self.sct = sct_instance
        self.temp_screenshot_path = "temp_screenshot.png"

    def select_roi(self):
        """
        [æ ¸å¿ƒä¿®æ”¹] å½»åº•åˆ†ç¦»æˆªå›¾å’ŒGUIæ“ä½œï¼Œæ ¹é™¤æ­»é”ã€‚
        """
        print("\nå‡†å¤‡é€‰æ‹©åŒºåŸŸ... æ­£åœ¨æˆªå–æ‚¨çš„ä¸»å±å¹•...")
        
        # æ­¥éª¤1ï¼šä»…æˆªå›¾ï¼Œå¹¶ç«‹åˆ»ä¿å­˜åˆ°æ–‡ä»¶
        try:
            full_screenshot = ImageGrab.grab()
            full_screenshot.save(self.temp_screenshot_path)
            print("æˆªå›¾å·²ä¿å­˜ã€‚")
        except Exception as e:
            print(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            self.roi = None
            return

        # æ­¥éª¤2ï¼šä»æ–‡ä»¶åŠ è½½é™æ€å›¾ç‰‡ï¼Œå†è¿›è¡ŒGUIæ“ä½œ
        try:
            img_from_file = cv2.imread(self.temp_screenshot_path)
            if img_from_file is None:
                raise FileNotFoundError("æ— æ³•è¯»å–ä¿å­˜çš„æˆªå›¾æ–‡ä»¶ã€‚")

            window_name = "è¯·ç”¨é¼ æ ‡æ‹–åŠ¨é€‰æ‹©Dinoæ¸¸æˆåŒºåŸŸ, ç„¶åæŒ‰ ENTER æˆ– SPACE é”®ç¡®è®¤"
            print(f"åœ¨å¼¹å‡ºçš„ '{window_name}' çª—å£ä¸­æ“ä½œ...")
            
            cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)
            cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
            
            roi_coords = cv2.selectROI(window_name, img_from_file, fromCenter=False, showCrosshair=True)
            cv2.destroyAllWindows()
            
            # æ­¥éª¤3ï¼šæ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(self.temp_screenshot_path)

            if sum(roi_coords) == 0:
                print("âŒ ROI selection cancelled.")
                self.roi = None
                return
                
            x, y, w, h = roi_coords
            self.roi = {"top": y, "left": x, "width": w, "height": h}
            print(f"âœ… æ¸¸æˆåŒºåŸŸé€‰æ‹©æˆåŠŸ: {self.roi}")
        
        except Exception as e:
            print(f"âŒ é€‰æ‹©åŒºåŸŸæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            self.roi = None
            if os.path.exists(self.temp_screenshot_path):
                os.remove(self.temp_screenshot_path) # ç¡®ä¿æ¸…ç†
            return
            
    def capture(self) -> np.ndarray:
        if not self.roi:
            return None
        
        sct_img = self.sct.grab(self.roi)
        return cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGRA2BGR)
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\world_modeling\__init__.py ---


----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\world_modeling\state.py ---

# src/world_modeling/state.py (V5 - æ„ŸçŸ¥æ—¶é—´)

class GameState:
    def __init__(self):
        self.dino_box = None
        self.obstacles = []
        self.dt = 1/30 # é»˜è®¤å€¼

    def update(self, detections, dt): # æ¥æ”¶dt
        self.dino_box = None
        self.obstacles = []
        self.dt = dt # ä¿å­˜çœŸå®çš„dt
        
        for box, class_name in detections:
            if class_name == 'dino-player':
                self.dino_box = box
            else:
                self.obstacles.append((box, class_name))
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\src\world_modeling\world_model.py ---

# src/world_modeling/world_model.py (V3 - å¢å¼ºè¯Šæ–­ä¸æ³¨é‡Š)
import numpy as np
from filterpy.kalman import UnscentedKalmanFilter
from filterpy.kalman import MerweScaledSigmaPoints
import time

class UKFWorldModel:
    """
    ä¸–ç•Œå»ºæ¨¡å±‚ï¼šä½¿ç”¨æ— è¿¹å¡å°”æ›¼æ»¤æ³¢å™¨ (Unscented Kalman Filter, UKF)
    æ¥ä¼°è®¡æœ€æ¥è¿‘éšœç¢ç‰©çš„çŠ¶æ€ï¼ˆä½ç½®å’Œé€Ÿåº¦ï¼‰ã€‚
    è¿™æ˜¯ L2/L3 çº§æ™ºèƒ½ä½“ä¸­ï¼Œä»çº¯ç²¹çš„â€œæ„ŸçŸ¥â€è¿ˆå‘â€œç†è§£â€çš„å…³é”®ä¸€æ­¥ã€‚
    """
    def __init__(self): 
        self.tracker = None

    def _create_tracker(self, initial_x, dt):
        """
        åˆå§‹åŒ–UKFè¿½è¸ªå™¨ã€‚
        çŠ¶æ€å‘é‡ (ukf.x): [position, velocity]
        è§‚æµ‹å‘é‡ (ukf.z): [position]
        """
        # MerweScaledSigmaPoints æ˜¯ä¸€ç§ç”¨äºUKFçš„é‡‡æ ·ç‚¹ç”Ÿæˆç­–ç•¥
        points = MerweScaledSigmaPoints(n=2, alpha=0.1, beta=2.0, kappa=-1)
        
        # dim_x=2: çŠ¶æ€å‘é‡ç»´åº¦ä¸º2 (ä½ç½®, é€Ÿåº¦)
        # dim_z=1: è§‚æµ‹å‘é‡ç»´åº¦ä¸º1 (åªè§‚æµ‹ä½ç½®)
        ukf = UnscentedKalmanFilter(dim_x=2, dim_z=1, dt=dt, hx=self._hx, fx=self._fx, points=points)

        # åˆå§‹çŠ¶æ€ï¼šä½ç½®ä¸ºé¦–æ¬¡è§‚æµ‹åˆ°çš„ä½ç½®ï¼Œé€Ÿåº¦ä¸º0
        ukf.x = np.array([initial_x, 0.0])
        
        # çŠ¶æ€åæ–¹å·®çŸ©é˜µ (P): è¡¨ç¤ºçŠ¶æ€ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚
        # ä½ç½®ä¸ç¡®å®šæ€§è¾ƒå°ï¼Œé€Ÿåº¦ä¸ç¡®å®šæ€§è¾ƒå¤§ã€‚
        ukf.P = np.diag([100.0, 5000.0]) 
        
        # æµ‹é‡å™ªå£°åæ–¹å·® (R): è¡¨ç¤ºæˆ‘ä»¬å¯¹è§‚æµ‹å€¼çš„ä¿¡ä»»ç¨‹åº¦ã€‚å€¼è¶Šå°ï¼Œè¶Šä¿¡ä»»è§‚æµ‹ã€‚
        ukf.R = np.diag([25.0]) 
        
        # è¿‡ç¨‹å™ªå£°åæ–¹å·® (Q): è¡¨ç¤ºæˆ‘ä»¬å¯¹çŠ¶æ€è½¬ç§»æ¨¡å‹(_fx)çš„ä¿¡ä»»ç¨‹åº¦ã€‚
        # å€¼è¶Šå¤§ï¼Œè¡¨ç¤ºæˆ‘ä»¬è®¤ä¸ºç³»ç»ŸåŠ¨æ€å˜åŒ–è¶Šå¿«ï¼Œå¯¹æ¨¡å‹çš„é¢„æµ‹è¶Šä¸ä¿¡ä»»ã€‚
        ukf.Q = np.array([[0.1, 0.0], [0.0, 10.0]])
        
        return ukf

    def _fx(self, x, dt):
        """
        çŠ¶æ€è½¬ç§»å‡½æ•°ï¼šæ ¹æ®ç‰©ç†æ¨¡å‹é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»çš„çŠ¶æ€ã€‚
        è¿™é‡Œä½¿ç”¨ç®€å•çš„åŒ€é€Ÿè¿åŠ¨æ¨¡å‹ã€‚
        pos_next = pos + vel * dt
        vel_next = vel
        """
        F = np.array([[1, dt], [0, 1]])
        return F @ x

    def _hx(self, x):
        """
        è§‚æµ‹å‡½æ•°ï¼šå°†çŠ¶æ€å‘é‡æ˜ å°„åˆ°è§‚æµ‹ç©ºé—´ã€‚
        æˆ‘ä»¬åªèƒ½è§‚æµ‹åˆ°ä½ç½®ï¼Œæ‰€ä»¥åªè¿”å›çŠ¶æ€å‘é‡çš„ç¬¬ä¸€ä¸ªå…ƒç´ ã€‚
        """
        return np.array([x[0]])

    def update(self, obstacle_measurement, dt):
        """
        ç”¨æ–°çš„è§‚æµ‹æ•°æ®æ›´æ–°ä¸–ç•Œæ¨¡å‹ã€‚
        """
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°éšœç¢ç‰©ï¼Œé‡ç½®è¿½è¸ªå™¨ã€‚
        # è¿™æ˜¯ä¸ºäº†é˜²æ­¢åœ¨éšœç¢ç‰©æ¶ˆå¤±åï¼Œæ¨¡å‹ç»§ç»­é”™è¯¯åœ°é¢„æµ‹å…¶ä½ç½®ã€‚
        if obstacle_measurement is None:
            self.tracker = None
            return

        # æˆ‘ä»¬åªå…³å¿ƒéšœç¢ç‰©çš„xåæ ‡
        x_pos = obstacle_measurement[0][0]

        # å¦‚æœè¿½è¸ªå™¨æœªåˆå§‹åŒ–ï¼Œåˆ™ç”¨å½“å‰è§‚æµ‹å€¼åˆ›å»ºæ–°çš„è¿½è¸ªå™¨ã€‚
        if self.tracker is None:
            self.tracker = self._create_tracker(x_pos, dt)
        
        # UKFæ ‡å‡†æ­¥éª¤ï¼šé¢„æµ‹ -> æ›´æ–°
        self.tracker.predict(dt=dt)
        self.tracker.update(np.array([x_pos]))
        
    def get_state(self):
        """

        è·å–å½“å‰ä¸–ç•Œæ¨¡å‹çš„çŠ¶æ€ï¼ˆä½ç½®å’Œé€Ÿåº¦ï¼‰ã€‚
        """
        if self.tracker is None:
            return None, None
        
        pos = self.tracker.x[0]
        # é€Ÿåº¦å–è´Ÿå€¼ï¼Œå› ä¸ºéšœç¢ç‰©åœ¨å±å¹•ä¸Šå‘å·¦ç§»åŠ¨ï¼ˆxåæ ‡å‡å°ï¼‰ï¼Œ
        # ä½†æˆ‘ä»¬é€šå¸¸å°†æ¸¸æˆâ€œé€Ÿåº¦â€ç†è§£ä¸ºä¸€ä¸ªæ­£å€¼ã€‚
        vel = -self.tracker.x[1] 
        return pos, vel
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\1_detection_pipeline\train_detector.py ---

import sys
from pathlib import Path
from ultralytics import YOLO
import torch

# [æ ¸å¿ƒä¿®æ”¹] è®©è„šæœ¬èƒ½å¤Ÿæ„ŸçŸ¥åˆ°é¡¹ç›®çš„æ ¹ç›®å½•
# è¿™ä½¿å¾—æ— è®ºæˆ‘ä»¬ä»å“ªé‡Œè¿è¡Œè¿™ä¸ªè„šæœ¬ï¼Œå®ƒéƒ½èƒ½æ­£ç¡®æ‰¾åˆ°å…¶ä»–æ–‡ä»¶
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

def main():
    """
    è®­ç»ƒå¹¶å¯¼å‡ºYOLOv8ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # --- [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿è·¯å¾„çš„ç¨³å¥æ€§ ---
    # å®šä¹‰æ‰€æœ‰è¾“å…¥æ–‡ä»¶çš„è·¯å¾„
    base_model_path = project_root / "_inputs" / "base_models" / "yolov8n.pt"
    data_config_path = project_root / "training_pipelines" / "1_detection_pipeline" / "data.yaml"
    
    # å®šä¹‰æ‰€æœ‰è¾“å‡ºçš„æ ¹ç›®å½•
    output_dir = project_root / "training_runs"
    
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"åŠ è½½æ•°æ®é…ç½®: {data_config_path}")
    print(f"è®­ç»ƒäº§å‡ºå°†ä¿å­˜è‡³: {output_dir}")

    # 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = YOLO(base_model_path)

    # 2. è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹ç›®æ ‡æ£€æµ‹æ¨¡å‹è®­ç»ƒ...")
    model.train(
        data=str(data_config_path), # ç¡®ä¿ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²
        epochs=50,
        imgsz=640,
        batch=8,
        project=str(output_dir), # [æ ¸å¿ƒä¿®æ”¹] æ§åˆ¶è¾“å‡ºç›®å½•
        name='detection_run'     # å®šä¹‰æœ¬æ¬¡å®éªŒçš„åç§°
    )
    print("è®­ç»ƒå®Œæˆã€‚")

    # 3. å¯¼å‡ºæ¨¡å‹ä¸ºONNX
    # åŠ è½½è®­ç»ƒå‡ºçš„æœ€ä½³æ¨¡å‹
    best_model_path = model.trainer.best
    model = YOLO(best_model_path)
    
    print(f"æ­£åœ¨ä» {best_model_path} å¯¼å‡ºæœ€ä½³æ¨¡å‹ä¸ºONNX...")
    # å¯¼å‡ºçš„ONNXæ–‡ä»¶ä¼šè‡ªåŠ¨ä¿å­˜åœ¨ä¸ best.pt ç›¸åŒçš„ç›®å½•ä¸‹
    model.export(format='onnx', opset=12) 
    print("å¯¼å‡ºå®Œæˆã€‚")
    print(f"è¯·æ‰‹åŠ¨å°†æœ€ç»ˆçš„ .onnx æ–‡ä»¶ç§»åŠ¨åˆ° 'models/detection/' ç›®å½•ä¸‹ï¼Œå¹¶é‡å‘½åä¸º 'dino_detector.onnx'ã€‚")


if __name__ == '__main__':
    main()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\2_classification_pipeline\1_generate_dataset.py ---

import sys
from pathlib import Path
import os
import random
from PIL import Image

# [æ ¸å¿ƒä¿®æ”¹] è®©è„šæœ¬èƒ½å¤Ÿæ„ŸçŸ¥åˆ°é¡¹ç›®çš„æ ¹ç›®å½•
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä¹‰è¾“å…¥å’Œè¾“å‡º ---
CONFIG = {
    "assets_path": project_root / "_assets",
    "output_path": project_root / "data" / "classification_data",
    "image_size": (100, 100),
    "background_color": (247, 247, 247),
    "images_per_class": 500,
    "min_scale": 0.7,
    "max_scale": 1.0,
}

def generate_classification_dataset():
    """
    [æ ¸å¿ƒä¿®æ­£] ç›´æ¥ã€å¿ å®åœ°ä½¿ç”¨ _assets æ–‡ä»¶å¤¹ä¸­çš„æ¯ä¸€ä¸ªåŸå§‹ png æ–‡ä»¶æ¥ç”Ÿæˆæ•°æ®ã€‚
    """
    assets_path = Path(CONFIG["assets_path"])
    output_path = Path(CONFIG["output_path"])

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path.mkdir(exist_ok=True, parents=True)

    print(f"è¾“å…¥ç´ æç›®å½•: {assets_path}")
    print(f"è¾“å‡ºæ•°æ®é›†ç›®å½•: {output_path}")

    if not assets_path.exists():
        print(f"âŒ é”™è¯¯ï¼šèµ„äº§æ–‡ä»¶å¤¹ '{assets_path}' ä¸å­˜åœ¨ï¼")
        return

    asset_files = list(assets_path.glob("*.png"))
    if not asset_files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ '{assets_path}' ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•.pngæ–‡ä»¶ï¼")
        return

    print(f"ğŸ” æ‰¾åˆ°äº† {len(asset_files)} ä¸ªèµ„äº§æ–‡ä»¶ã€‚å¼€å§‹ç”Ÿæˆæ•°æ®é›†...")

    for asset_file in asset_files:
        if 'dino-player' in asset_file.stem:
            print(f"  -> è·³è¿‡ééšœç¢ç‰©ç´ æ: {asset_file.name}")
            continue

        class_name = asset_file.stem
        class_dir = output_path / class_name
        class_dir.mkdir(parents=True, exist_ok=True)

        print(f"  -> æ­£åœ¨ä¸ºç±»åˆ« '{class_name}' ç”Ÿæˆ {CONFIG['images_per_class']} å¼ å›¾ç‰‡...")
        
        try:
            asset_img = Image.open(asset_file).convert("RGBA")
        except Exception as e:
            print(f"   âš ï¸ è­¦å‘Šï¼šæ— æ³•æ‰“å¼€æ–‡ä»¶ {asset_file}ï¼Œå·²è·³è¿‡ã€‚é”™è¯¯: {e}")
            continue

        for i in range(CONFIG["images_per_class"]):
            background = Image.new("RGBA", CONFIG["image_size"], CONFIG["background_color"])
            
            max_allowed_scale_w = CONFIG["image_size"][0] / asset_img.width
            max_allowed_scale_h = CONFIG["image_size"][1] / asset_img.height
            final_max_scale = min(max_allowed_scale_w, max_allowed_scale_h, CONFIG["max_scale"])
            
            if final_max_scale < CONFIG["min_scale"]:
                scale = final_max_scale
            else:
                scale = random.uniform(CONFIG["min_scale"], final_max_scale)

            new_width = int(asset_img.width * scale)
            new_height = int(asset_img.height * scale)
            
            if new_width <= 0 or new_height <= 0: continue

            resized_asset = asset_img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            max_x = CONFIG["image_size"][0] - new_width
            max_y = CONFIG["image_size"][1] - new_height
            
            paste_x = random.randint(0, max(0, max_x))
            paste_y = random.randint(0, max(0, max_y))

            background.paste(resized_asset, (paste_x, paste_y), resized_asset)
            
            final_image = background.convert("RGB")
            output_file_path = class_dir / f"{i:04d}.png"
            final_image.save(output_file_path)

    print(f"\nâœ… æ•°æ®é›†ç”Ÿæˆå®Œæ¯•ï¼å·²ä¿å­˜è‡³ '{output_path}' ç›®å½•ã€‚")

if __name__ == "__main__":
    generate_classification_dataset()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\2_classification_pipeline\2_train_classifier.py ---

# training_pipelines/2_classification_pipeline/2_train_classifier.py (V1.2 - æœ€ç»ˆä¿®æ­£ç‰ˆ)
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import time
import copy # å¯¼å…¥copyæ¨¡å—

# [æ ¸å¿ƒä¿®æ”¹] è®©è„šæœ¬èƒ½å¤Ÿæ„ŸçŸ¥åˆ°é¡¹ç›®çš„æ ¹ç›®å½•
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [æ ¸å¿ƒä¿®æ”¹] ä½¿ç”¨ç»å¯¹è·¯å¾„å®šä¹‰è¾“å…¥å’Œè¾“å‡º ---
CONFIG = {
    "dataset_path": project_root / "data" / "classification_data",
    "output_model_dir": project_root / "models" / "classification",
    "output_model_name": "dino_classifier.pth",
    "num_epochs": 15,
    "batch_size": 32,
    "learning_rate": 0.001,
}

def train_classifier():
    """
    ä½¿ç”¨è¿ç§»å­¦ä¹ è®­ç»ƒä¸€ä¸ªEfficientNetV2æ¨¡å‹æ¥åˆ†ç±»æé¾™æ¸¸æˆä¸­çš„éšœç¢ç‰©ã€‚
    """
    print("å¼€å§‹è®­ç»ƒå›¾åƒåˆ†ç±»å™¨...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(CONFIG["output_model_dir"])
    output_dir.mkdir(exist_ok=True, parents=True)

    # 1. æ•°æ®é¢„å¤„ç†å’ŒåŠ è½½
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    dataset_path = Path(CONFIG["dataset_path"])
    print(f"åŠ è½½æ•°æ®é›†: {dataset_path}")
    if not dataset_path.exists() or not any(dataset_path.iterdir()):
        print(f"âŒ é”™è¯¯: æ•°æ®é›†ç›®å½• '{dataset_path}' ä¸å­˜åœ¨æˆ–ä¸ºç©º!")
        print("è¯·å…ˆè¿è¡Œ 1_generate_dataset.py æ¥ç”Ÿæˆæ•°æ®ã€‚")
        return
        
    full_dataset = datasets.ImageFolder(dataset_path)
    
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    # --- [æ ¸å¿ƒä¿®æ­£] ä½¿ç”¨æ·±æ‹·è´è§£å†³Subsetå…±äº«transformçš„é—®é¢˜ ---
    val_dataset.dataset = copy.deepcopy(full_dataset)
    train_dataset.dataset.transform = data_transforms['train']
    val_dataset.dataset.transform = data_transforms['val']
    # ----------------------------------------------------

    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=4),
        'val': DataLoader(val_dataset, batch_size=CONFIG["batch_size"], shuffle=False, num_workers=4)
    }
    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}
    class_names = full_dataset.classes
    num_classes = len(class_names)
    print(f"å‘ç° {num_classes} ä¸ªç±»åˆ«: {class_names}")

    # 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶ä¿®æ”¹æœ€åä¸€å±‚
    model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)
    for param in model.parameters():
        param.requires_grad = False
        
    num_ftrs = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(num_ftrs, num_classes)
    model = model.to(device)
    print("âœ… EfficientNetV2 æ¨¡å‹åŠ è½½å¹¶ä¿®æ”¹å®Œæˆã€‚")

    # 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.classifier.parameters(), lr=CONFIG["learning_rate"])

    # 4. è®­ç»ƒå¾ªç¯
    since = time.time()
    best_model_wts = model.state_dict()
    best_acc = 0.0

    for epoch in range(CONFIG["num_epochs"]):
        print(f'Epoch {epoch+1}/{CONFIG["num_epochs"]}')
        print('-' * 10)
        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            running_loss = 0.0
            running_corrects = 0
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = model.state_dict()
                print(f'ğŸ‰ æ–°çš„æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:.4f}')
    
    time_elapsed = time.time() - since
    print(f'è®­ç»ƒå®Œæˆï¼Œè€—æ—¶ {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'æœ€ä½³éªŒè¯é›†å‡†ç¡®ç‡: {best_acc:4f}')
    
    # 5. ä¿å­˜æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_wts)
    save_path = output_dir / CONFIG["output_model_name"]
    torch.save({
        'model_state_dict': model.state_dict(),
        'class_names': class_names
    }, save_path)
    print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

if __name__ == '__main__':
    train_classifier()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\3_policy_pipeline\1_collect_data.py ---

# training_pipelines/3_policy_pipeline/1_collect_data.py (V1.1 - æœ€ç»ˆè·¯å¾„ä¿®æ­£ç‰ˆ)
import sys
from pathlib import Path
import cv2
import mss
import time
import numpy as np
import pynput
import uuid
import os # å¯¼å…¥osæ¨¡å—

# [æ ¸å¿ƒ] ä¿®æ­£Pythonæ¨¡å—æœç´¢è·¯å¾„ï¼Œç¡®ä¿æ— è®ºä»å“ªé‡Œè¿è¡Œéƒ½èƒ½æ‰¾åˆ°src
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.perception.detector import AdvancedDetector
from src.world_modeling.state import GameState
from src.world_modeling.world_model import UKFWorldModel
from src.utils.screen_manager import ScreenManager

# --- [æ ¸å¿ƒä¿®æ­£] ä¸¥æ ¼å¯¹é½æœ€ç»ˆç›®å½•ç»“æ„ ---
RAW_DATA_DIR = project_root / "data" / "policy_data" / "raw"
RAW_DATA_DIR.mkdir(exist_ok=True, parents=True)


ACTION_MAP = {'nothing': np.array([1, 0, 0], dtype=np.float32), 'jump': np.array([0, 1, 0], dtype=np.float32), 'duck': np.array([0, 0, 1], dtype=np.float32)}
current_action = ACTION_MAP['nothing']
def on_press(key):
    global current_action
    if key == pynput.keyboard.Key.space: current_action = ACTION_MAP['jump']
    elif key == pynput.keyboard.Key.down: current_action = ACTION_MAP['duck']
def on_release(key):
    global current_action
    if key in [pynput.keyboard.Key.space, pynput.keyboard.Key.down]: current_action = ACTION_MAP['nothing']
def normalize(value, min_val, max_val):
    if (max_val - min_val) == 0: return 0.0
    return max(0.0, min(1.0, (value - min_val) / (max_val - min_val)))
def build_state_vector(game_state, world_model):
    dino_box = game_state.dino_box
    obstacles = game_state.obstacles
    _, speed = world_model.get_state()
    MAX_SPEED, MAX_DIST, MAX_HEIGHT, MAX_WIDTH = 1000, 800, 100, 100
    state = [normalize(speed if speed else 0, 0, MAX_SPEED)]
    if dino_box is None: return None
    obstacles.sort(key=lambda o: o[0][0])
    for i in range(2):
        if i < len(obstacles):
            obs_box, _ = obstacles[i]
            dist = obs_box[0] - dino_box[2]
            state.extend([normalize(dist, 0, MAX_DIST), normalize(obs_box[3] - obs_box[1], 0, MAX_HEIGHT), normalize(obs_box[2] - obs_box[0], 0, MAX_WIDTH)])
        else:
            state.extend([1.0, 0.0, 0.0])
    return np.array(state, dtype=np.float32)

def main():
    print("å‡†å¤‡å¼€å§‹é‡‡é›†ä¸“å®¶æ•°æ®...")
    listener = pynput.keyboard.Listener(on_press=on_press, on_release=on_release)
    listener.start()
    with mss.mss() as sct:
        # --- [æ ¸å¿ƒä¿®æ­£] ä¸¥æ ¼å¯¹é½æœ€ç»ˆæ¨¡å‹è·¯å¾„ ---
        detector = AdvancedDetector(
            yolo_model_path=str(project_root / "models" / "detection" / "dino_detector.onnx"),
            classifier_model_path=str(project_root / "models" / "classification" / "dino_classifier.pth")
        )
        # --------------------------------------
        
        game_state, world_model, screen_manager = GameState(), UKFWorldModel(), ScreenManager(sct)
        screen_manager.select_roi()
        if screen_manager.roi is None: return

        print(f"å¼€å§‹é‡‡é›†... æ•°æ®å°†ä¿å­˜è‡³ {RAW_DATA_DIR}")
        print("æŒ‰ 'q' é”®åœæ­¢ã€‚")
        prev_time = time.time()
        while True:
            current_time = time.time()
            dt = current_time - prev_time
            if dt == 0: dt = 1/60
            prev_time = current_time
            
            img = screen_manager.capture()
            if img is None: continue
            
            detections = detector.detect(img, yolo_class_names=['bird', 'cactus', 'dino'])
            game_state.update(detections, dt)
            
            closest_obs = min(game_state.obstacles, key=lambda o: o[0][0]) if game_state.obstacles else None
            world_model.update(closest_obs, dt)
            state_vector = build_state_vector(game_state, world_model)
            
            if state_vector is not None:
                filename = RAW_DATA_DIR / f"{uuid.uuid4()}.npz"
                np.savez(filename, state=state_vector, action=current_action)

            cv2.imshow("Data Collection", img)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    listener.stop()
    cv2.destroyAllWindows()
    print(f"æ•°æ®é‡‡é›†å®Œæˆï¼")

if __name__ == "__main__":
    main()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\3_policy_pipeline\2_process_data.py ---

# training_pipelines/3_policy_pipeline/2_process_data.py
import sys
from pathlib import Path
import numpy as np
import os
from tqdm import tqdm

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [æ ¸å¿ƒä¿®æ”¹] æ›´æ–°è¾“å…¥å’Œè¾“å‡ºç›®å½• ---
RAW_DATA_DIR = project_root / "data" / "policy_data" / "raw"
PROCESSED_DATA_DIR = project_root / "data" / "policy_data" / "processed"
PROCESSED_DATA_DIR.mkdir(exist_ok=True, parents=True)
PROCESSED_DATA_PATH = PROCESSED_DATA_DIR / "iql_dataset.npz"

SURVIVAL_REWARD, CRASH_PENALTY, EPISODE_TIMEOUT_SECONDS = 0.1, -100.0, 2.0

def process_raw_data():
    print(f"ğŸ” å¼€å§‹ä» {RAW_DATA_DIR} è¯»å–åŸå§‹æ•°æ®æ–‡ä»¶...")
    files = sorted(list(RAW_DATA_DIR.glob("*.npz")), key=os.path.getmtime)
    if not files:
        print(f"âŒ é”™è¯¯ï¼šåœ¨ {RAW_DATA_DIR} æ‰¾ä¸åˆ° .npz æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ 1_collect_data.pyã€‚")
        return
    print(f"âœ… æ‰¾åˆ°äº† {len(files)} ä¸ªæ•°æ®ç‚¹ã€‚")
    
    observations, actions, rewards, terminals, next_observations = [], [], [], [], []
    
    print("ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®ï¼Œæ„å»ºè½¬æ¢åºåˆ— (s, a, r, s', d)...")
    for i in tqdm(range(len(files) - 1), desc="Processing Transitions"):
        current_data, next_data = np.load(files[i]), np.load(files[i+1])
        done = (os.path.getmtime(files[i+1]) - os.path.getmtime(files[i])) > EPISODE_TIMEOUT_SECONDS
        
        observations.append(current_data['state'])
        actions.append(np.argmax(current_data['action']))
        rewards.append(CRASH_PENALTY if done else SURVIVAL_REWARD)
        terminals.append(done)
        next_observations.append(next_data['state'])


    if files:
        last_data = np.load(files[-1])
        observations.append(last_data['state'])
        actions.append(np.argmax(last_data['action']))
        rewards.append(CRASH_PENALTY)
        terminals.append(True)
        next_observations.append(np.zeros_like(last_data['state']))
        
    print(f"ğŸ’¾ æ•°æ®å¤„ç†å®Œæˆï¼Œæ­£åœ¨ä¿å­˜åˆ° {PROCESSED_DATA_PATH}...")
    np.savez(
        PROCESSED_DATA_PATH,
        observations=np.array(observations, dtype=np.float32),
        actions=np.array(actions, dtype=np.uint8),
        rewards=np.array(rewards, dtype=np.float32),
        terminals=np.array(terminals, dtype=np.float32),
        next_observations=np.array(next_observations, dtype=np.float32),
    )
    print("ğŸ‰ æˆåŠŸï¼IQLæ•°æ®é›†å·²å‡†å¤‡å°±ç»ªã€‚")

if __name__ == "__main__":
    process_raw_data()
----------------------------------------------------

--- æ–‡ä»¶: C:\dino_ai\training_pipelines\3_policy_pipeline\3_train_and_export.py ---

# training_pipelines/3_policy_pipeline/3_train_and_export.py
import sys
from pathlib import Path
import numpy as np
import torch
from d3rlpy.algos import DiscreteCQLConfig
from d3rlpy.datasets import MDPDataset
from d3rlpy.logging import CombineAdapterFactory, FileAdapterFactory, TensorboardAdapterFactory
from d3rlpy.metrics import TDErrorEvaluator, AverageValueEstimationEvaluator

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [æ ¸å¿ƒä¿®æ”¹] æ›´æ–°æ‰€æœ‰è·¯å¾„ ---
DATASET_PATH = project_root / "data" / "policy_data" / "processed" / "iql_dataset.npz"
LOG_DIR_BASE = project_root / "training_runs" # å°†æ‰€æœ‰è®­ç»ƒæ—¥å¿—ç»Ÿä¸€å­˜æ”¾
EXPERIMENT_NAME = "policy_run"
ONNX_MODEL_PATH = project_root / "models" / "policy" / "dino_policy.onnx"
ONNX_MODEL_PATH.parent.mkdir(exist_ok=True)

TRAIN_STEPS, STEPS_PER_EPOCH, SAVE_INTERVAL_IN_EPOCHS = 200000, 10000, 2

def main():
    print(f"ğŸ§  å¼€å§‹æ‰§è¡Œâ€œä¸€é”®å¼â€è®­ç»ƒä¸å¯¼å‡ºæµç¨‹...")
    
    # 1. åŠ è½½æ•°æ®é›†
    try:
        data = np.load(DATASET_PATH)
        dataset = MDPDataset(
            observations=data['observations'], actions=data['actions'],
            rewards=data['rewards'], terminals=data['terminals'],
        )
        print(f"âœ… æˆåŠŸåŠ è½½å¹¶æ„å»ºæ•°æ®é›†: {DATASET_PATH}")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®é›†æ–‡ä»¶ {DATASET_PATH}ã€‚è¯·å…ˆè¿è¡Œ 2_process_data.pyã€‚")
        return
        
    # 2. é…ç½® DiscreteCQL
    cql_config = DiscreteCQLConfig(
        batch_size=256, gamma=0.99, learning_rate=1e-4, alpha=1.0, n_critics=2,
    )

    # 3. åˆ›å»ºå®ä¾‹
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    cql = cql_config.create(device=device)

    # 4. é…ç½®æ—¥å¿—
    logger = CombineAdapterFactory([
        FileAdapterFactory(root_dir=str(LOG_DIR_BASE)),
        TensorboardAdapterFactory(root_dir=str(LOG_DIR_BASE))
    ])
    
    # 5. é…ç½®è¯„ä¼°å™¨
    evaluators = { 'td_error': TDErrorEvaluator(), 'avg_value': AverageValueEstimationEvaluator() }
    
    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ æ­£åœ¨å¯åŠ¨è®­ç»ƒ... ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯åŠ¨TensorBoardæ¥å®æ—¶ç›‘æ§ï¼š")
    print(f"   tensorboard --logdir {LOG_DIR_BASE}")
    
    cql.fit(
        dataset, n_steps=TRAIN_STEPS, n_steps_per_epoch=STEPS_PER_EPOCH,
        save_interval=SAVE_INTERVAL_IN_EPOCHS, logger_adapter=logger,
        experiment_name=EXPERIMENT_NAME, with_timestamp=False, evaluators=evaluators,
    )
    
    # 7. å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
    final_model_dir = LOG_DIR_BASE / EXPERIMENT_NAME
    print(f"ğŸ’¾ è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨å°†æœ€ç»ˆç­–ç•¥å¯¼å‡ºä¸º ONNX æ ¼å¼åˆ° {ONNX_MODEL_PATH}...")
    cql.save_policy(str(ONNX_MODEL_PATH))
    
    print("\n" + "="*50)
    print("ğŸ‰ğŸ‰ğŸ‰ â€œä¸€é”®å¼â€æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼ä½ çš„æœ€ä¼˜ä¸“å®¶å¤§è„‘å·²æˆåŠŸé“¸é€ ï¼ ğŸ‰ğŸ‰ğŸ‰")
    print(f"æœ€ç»ˆäº§å‡º: {ONNX_MODEL_PATH}")
    print("ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ»¡æ€€ä¿¡å¿ƒåœ°å»è¿è¡Œæœ€ç»ˆçš„ run_bot.py äº†ï¼")
    print("="*50)

if __name__ == "__main__":
    main()
----------------------------------------------------

====================================================
  [AI åˆ†ææ•°æ®ç»“æŸ]
====================================================
