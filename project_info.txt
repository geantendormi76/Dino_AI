=== 核心目录结构树 (仅文件夹和.py文件): ===

C:\dino_ai
|   |-- py结构树.py
|   |-- run_bot.py
    |-- data
        |-- classification_data
            |-- bird-style1-02
            |-- bird-style1-03
            |-- cactus-large_double
            |-- cactus-large_single
            |-- cactus-small_double
            |-- cactus-small_single
            |-- cactus-small_triple
        |-- detection_data
            |-- images
            |-- labels
        |-- policy_data
            |-- processed
            |-- raw
    |-- models
        |-- classification
        |-- detection
            |-- onnx_cache
        |-- onnx_cache
        |-- policy
    |-- src
    |   |-- __init__.py
    |   |-- brain.py
    |   |-- config.py
    |   |-- contracts.py
    |   |-- state_builder.py
        |-- controls
        |   |-- __init__.py
        |   |-- agent.py
        |-- perception
        |   |-- __init__.py
        |   |-- detector.py
        |-- utils
        |   |-- __init__.py
        |   |-- screen_manager.py
        |-- world_modeling
        |   |-- __init__.py
        |   |-- state.py
        |   |-- world_model.py
    |-- training_pipelines
        |-- 1_detection_pipeline
        |   |-- train_detector.py
        |-- 2_classification_pipeline
        |   |-- 1_generate_dataset.py
        |   |-- 2_train_classifier.py
        |-- 3_policy_pipeline
        |   |-- 1_collect_data.py
        |   |-- 2_process_data.py
        |   |-- 3_train_and_export.py
    |-- _assets
    |-- _inputs
        |-- base_models
    |-- 训练数据图

=== 核心Python文件 (.py) 列表 (绝对路径): ===

C:\dino_ai\py结构树.py
C:\dino_ai\run_bot.py
C:\dino_ai\src\__init__.py
C:\dino_ai\src\brain.py
C:\dino_ai\src\config.py
C:\dino_ai\src\contracts.py
C:\dino_ai\src\controls\__init__.py
C:\dino_ai\src\controls\agent.py
C:\dino_ai\src\perception\__init__.py
C:\dino_ai\src\perception\detector.py
C:\dino_ai\src\state_builder.py
C:\dino_ai\src\utils\__init__.py
C:\dino_ai\src\utils\screen_manager.py
C:\dino_ai\src\world_modeling\__init__.py
C:\dino_ai\src\world_modeling\state.py
C:\dino_ai\src\world_modeling\world_model.py
C:\dino_ai\training_pipelines\1_detection_pipeline\train_detector.py
C:\dino_ai\training_pipelines\2_classification_pipeline\1_generate_dataset.py
C:\dino_ai\training_pipelines\2_classification_pipeline\2_train_classifier.py
C:\dino_ai\training_pipelines\3_policy_pipeline\1_collect_data.py
C:\dino_ai\training_pipelines\3_policy_pipeline\2_process_data.py
C:\dino_ai\training_pipelines\3_policy_pipeline\3_train_and_export.py

=== 核心Python文件代码内容: ===


--- 文件: C:\dino_ai\py结构树.py ---

import os
import sys

def generate_project_info(root_dir, output_filename="project_info.txt"):
    """
    生成项目信息报告，包括目录树和Python文件内容。
    该报告会过滤掉虚拟环境、缓存目录和常见的二进制文件，
    只保留对AI分析项目核心有用的信息。

    Args:
        root_dir (str): 要分析的根目录路径。
        output_filename (str): 报告输出的文件名。
    """
    try:
        # 确保根目录存在
        if not os.path.isdir(root_dir):
            print(f"错误：指定的路径不存在或不是一个目录: {root_dir}")
            return

        # 定义要忽略的目录和文件模式
        ignored_dirs = [
            'venv', '.venv', 'env', '__pycache__', '.git', '.idea',
            'node_modules', 'dist', 'build', '.vscode', '.pytest_cache',
            '__pycache__', 'site-packages' # 常见Python虚拟环境和缓存目录
        ]
        # 定义要忽略的文件扩展名（二进制文件、日志、配置文件等）
        ignored_extensions = [
            '.log', '.bin', '.dll', '.exe', '.so', '.dylib', '.DS_Store',
            '.pyc', '.pyo', '.ipynb_checkpoints', '.tmp', '.bak', '.swp',
            '.sqlite3', '.db', '.dat', '.json', '.yaml', '.yml', '.toml',
            '.xml', '.txt', '.md', '.csv', '.xlsx', '.xls', '.pdf',
            '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.svg', '.ico',
            '.zip', '.tar', '.gz', '.rar', '.7z', '.mp3', '.mp4', '.avi',
            # 您可能需要根据具体项目进一步扩展此列表
        ]

        with open(output_filename, 'w', encoding='utf-8') as f:

            # === 1. 生成目录结构树 (仅核心文件夹和.py文件) ===
            f.write("=== 核心目录结构树 (仅文件夹和.py文件): ===\n")
            f.write("\n")

            # 存储所有.py文件的路径，以便后续读取内容
            py_files_paths = []

            for dirpath, dirnames, filenames in os.walk(root_dir):
                # 过滤掉要忽略的目录
                dirnames[:] = [d for d in dirnames if d not in ignored_dirs]

                # 检查当前目录是否为被忽略的目录本身
                if os.path.basename(dirpath) in ignored_dirs and dirpath != root_dir:
                    continue

                # 计算当前目录的深度，用于树形缩进
                depth = dirpath[len(root_dir):].count(os.sep)
                indent = '    ' * depth
                
                # 写入当前目录名
                if dirpath == root_dir:
                    f.write(f"{root_dir}\n")
                else:
                    f.write(f"{indent}|-- {os.path.basename(dirpath)}\n")

                # 写入 .py 文件 (过滤掉忽略的文件扩展名)
                for filename in sorted(filenames):
                    if filename.endswith(".py") and not any(filename.endswith(ext) for ext in ignored_extensions):
                        f.write(f"{indent}|   |-- {filename}\n")
                        py_files_paths.append(os.path.join(dirpath, filename))
            f.write("\n")

            # === 2. 列出所有核心Python文件 (.py) 列表 ===
            f.write("=== 核心Python文件 (.py) 列表 (绝对路径): ===\n")
            f.write("\n")
            if py_files_paths:
                for py_file_path in sorted(py_files_paths):
                    f.write(f"{py_file_path}\n")
            else:
                f.write("没有找到任何核心.py文件。\n")
            f.write("\n")

            # === 3. 获取Python文件代码内容 ===
            f.write("=== 核心Python文件代码内容: ===\n")
            f.write("\n")

            if py_files_paths:
                for py_file_path in sorted(py_files_paths):
                    f.write(f"\n--- 文件: {py_file_path} ---\n")
                    f.write("\n")
                    try:
                        with open(py_file_path, 'r', encoding='utf-8') as py_f:
                            f.write(py_f.read())
                        f.write("\n")
                        f.write("----------------------------------------------------\n")
                    except Exception as e:
                        f.write(f"无法读取文件内容: {e}\n")
                        f.write("----------------------------------------------------\n")
            else:
                f.write("没有可读取内容的.py文件。\n")

            f.write("\n")
            f.write("====================================================\n")
            f.write("  [AI 分析数据结束]\n")
            f.write("====================================================\n")

        print(f"报告已成功生成并保存到: {output_filename}")
        print("操作完成。")

    except Exception as e:
        print(f"发生了一个错误: {e}")

if __name__ == "__main__":
    # 获取脚本所在的目录作为分析的根目录
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # 允许用户通过命令行参数指定路径，否则使用脚本所在目录
    if len(sys.argv) > 1:
        input_path = sys.argv[1]
        # 对用户输入的路径进行规范化处理，以应对Windows和WSL路径的混合情况
        # os.path.abspath 可以处理相对路径，并将其转换为绝对路径
        # os.path.normpath 可以规范化路径，处理'..' '.' 等
        # 注意：对于 \\wsl.localhost\Ubuntu-22.04\home\zhz 这样的路径，os模块通常能正确处理
        processed_path = os.path.normpath(os.path.abspath(input_path))
    else:
        processed_path = script_dir

    generate_project_info(processed_path)


----------------------------------------------------

--- 文件: C:\dino_ai\run_bot.py ---

# run_bot.py (V15 - 决策链条诊断 & 最终稳定版)
import sys
from pathlib import Path
import cv2
import mss
import time
import numpy as np
import onnxruntime as ort

project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root))

from src.perception.detector import AdvancedDetector
from src.world_modeling.state import GameState
from src.world_modeling.world_model import UKFWorldModel
from src.utils.screen_manager import ScreenManager
from src.controls.agent import GameAgent
from src.state_builder import build_state_vector

def main():
    print("🚀 启动 Dino AI (专家大脑 - 最终决战版)...")

    detector = None
    prev_time = time.time() 

    try:
        detector = AdvancedDetector(
            yolo_model_path="models/detection/dino_detector.onnx",
            classifier_model_path="models/classification/dino_classifier.pth"
        )
        game_state = GameState()
        world_model = UKFWorldModel()
        agent = GameAgent()
        screen_manager = ScreenManager(mss.mss())
    except Exception as e:
        print(f"❌ 错误：初始化核心模块失败: {e}")
        return

    # --- PyTorch Profiler 启用 (诊断结束后请注释或移除) ---
    # detector.enable_profiler(log_dir="runs/classifier_profiler_logs")
    # ----------------------------------------------------

    onnx_model_path = "models/policy/dino_policy.onnx"
    print(f"🧠 正在加载专家大脑: {onnx_model_path}")
    try:
        session_options = ort.SessionOptions()
        session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        trt_provider_options = {
            "trt_fp16_enable": True,
            "trt_cuda_graph_enable": False, # 暂时禁用 CUDA Graph，因为未实现 I/O Binding
            "trt_engine_cache_enable": True,
            "trt_engine_cache_path": str(project_root / "models" / "onnx_cache"),
            "trt_max_workspace_size": 2147483648, # 2GB 显存工作区
        }
        
        providers = [
            ("TensorrtExecutionProvider", trt_provider_options),
            "CUDAExecutionProvider",
            "CPUExecutionProvider",
        ]

        ort_session = ort.InferenceSession(
            onnx_model_path,
            sess_options=session_options,
            providers=providers
        )
        
        (project_root / "models" / "onnx_cache").mkdir(parents=True, exist_ok=True)

        print(f"✅ 专家大脑加载成功！使用设备: {ort_session.get_providers()}")
        print("注意：第一次运行可能较慢，TensorRT正在构建优化引擎并缓存。")

    except Exception as e:
        print(f"❌ 错误：加载ONNX模型失败: {e}")
        print("请检查ONNX Runtime GPU是否正确安装，以及TensorRT相关库是否可用。")
        print("错误详情:", e)
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler()
        return
    
    screen_manager.select_roi()
    if screen_manager.roi is None:
        print("🔴 未选择游戏区域，程序退出。")
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler() 
        return

    print("3秒后机器人将开始运行...")
    time.sleep(3)
    
    try:
        while True:
            frame_start_time = time.perf_counter()

            current_time = time.time()
            dt = current_time - prev_time
            if dt == 0: dt = 1/60
            prev_time = current_time 

            capture_start = time.perf_counter()
            img = screen_manager.capture()
            capture_end = time.perf_counter()
            if img is None: break
            
            detection_start = time.perf_counter()
            detections = detector.detect(img, yolo_class_names=['bird', 'cactus', 'dino'])
            detection_end = time.perf_counter()
            
            game_state_update_start = time.perf_counter()
            game_state.update(detections, dt)
            game_state_update_end = time.perf_counter()

            world_model_update_start = time.perf_counter()
            closest_obs = min(game_state.obstacles, key=lambda o: o[0][0]) if game_state.obstacles else None
            world_model.update(closest_obs, dt)
            world_model_update_end = time.perf_counter()
            
            state_build_start = time.perf_counter()
            state_vector = build_state_vector(game_state, world_model)
            state_build_end = time.perf_counter()

            # --- 诊断 GameState, WorldModel, StateVector 的内容 ---
            print(f"DEBUG_DECISION: Dino Box: {game_state.dino_box}")
            print(f"DEBUG_DECISION: Obstacles Count: {len(game_state.obstacles)} | Closest: {closest_obs}")
            pos, speed = world_model.get_state()
            print(f"DEBUG_DECISION: World Model State (Pos, Speed): ({pos:.2f}, {speed:.2f})" if pos is not None else "DEBUG_DECISION: World Model State: None")
            print(f"DEBUG_DECISION: State Vector: {state_vector.round(3) if state_vector is not None else 'None'}")
            
            action_index = 0
            inference_start = time.perf_counter()
            if state_vector is not None:
                input_state_tensor = np.expand_dims(state_vector, axis=0).astype(np.float32)

                input_name = ort_session.get_inputs()[0].name
                output_name = ort_session.get_outputs()[0].name 

                raw_action_result = [] 
                try:
                    raw_action_result = ort_session.run([output_name], {input_name: input_state_tensor})
                except Exception as e:
                    print(f"❌ ERROR: 决策模型 (dino_policy.onnx) ORT run 失败！详细错误: {e}")
                    print(f"  输入数据形状: {input_state_tensor.shape}, 类型: {input_state_tensor.dtype}")
                    raw_action_result = [] 

                if raw_action_result and len(raw_action_result) > 0:
                    action_output_tensor = raw_action_result[0]
                    if isinstance(action_output_tensor, np.ndarray) and action_output_tensor.size > 0:
                        action_index = int(action_output_tensor.flatten()[0]) 
                        if action_index not in [0, 1, 2]:
                            print(f"⚠️ 警告：决策模型返回了超出范围的动作索引: {action_index}。默认执行 '无操作'。")
                            action_index = 0
                    else:
                        print(f"⚠️ 警告：决策模型返回非 numpy 数组或空数组。原始结果: {raw_action_result}。默认执行 '无操作'。")
                        action_index = 0
                else:
                    print(f"⚠️ 警告：决策模型未返回任何结果。原始结果: {raw_action_result}。默认执行 '无操作'。")
                    action_index = 0

            inference_end = time.perf_counter()

            # --- 诊断决策动作和执行情况 ---
            print(f"DEBUG_DECISION: Chosen Action Index: {action_index}")

            action_execute_start = time.perf_counter()
            if action_index == 1:
                agent.jump(duration=0.05)
            elif action_index == 2:
                agent.duck()
            action_execute_end = time.perf_counter()

            frame_end_time = time.perf_counter()
            
            print(f"Frame Time: {((frame_end_time - frame_start_time)*1000):.2f}ms | "
                  f"Capture: {((capture_end - capture_start)*1000):.2f}ms | "
                  f"Detect: {((detection_end - detection_start)*1000):.2f}ms | "
                  f"GameState: {((game_state_update_end - game_state_update_start)*1000):.2f}ms | "
                  f"WorldModel: {((world_model_update_end - world_model_update_start)*1000):.2f}ms | "
                  f"StateBuild: {((state_build_end - state_build_start)*1000):.2f}ms | "
                  f"Inference: {((inference_end - inference_start)*1000):.2f}ms | "
                  f"ActionExecute: {((action_execute_end - action_execute_start)*1000):.2f}ms")

            debug_img = img.copy()
            for box, class_name in detections:
                x1, y1, x2, y2 = box
                color_map = {"dino": (0, 255, 0), "cactus": (0, 0, 255), "bird": (255, 0, 0)}
                color = next((c for k, c in color_map.items() if k in class_name), (0, 255, 255))
                cv2.rectangle(debug_img, (x1, y1), (x2, y2), color, 2)
                cv2.putText(debug_img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            _, speed = world_model.get_state()
            speed_text = f"Speed: {speed or 0:.0f}"
            cv2.putText(debug_img, speed_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)
            cv2.imshow("Dino AI - Expert Brain (Debug View)", debug_img)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    finally:
        if detector and hasattr(detector, 'disable_profiler') and callable(detector.disable_profiler):
            detector.disable_profiler() 
            
    cv2.destroyAllWindows()
    print("🤖 Dino AI 已停止运行。")
    
if __name__ == "__main__":
    main()
----------------------------------------------------

--- 文件: C:\dino_ai\src\__init__.py ---


----------------------------------------------------

--- 文件: C:\dino_ai\src\brain.py ---

# src/brain.py
import torch.nn as nn

class ImitationBrain(nn.Module):
    def __init__(self, input_size, output_size):
        super(ImitationBrain, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, output_size)
        )
    def forward(self, x):
        return self.network(x)
----------------------------------------------------

--- 文件: C:\dino_ai\src\config.py ---

# src/config.py

import torch
import onnxruntime

# --- 动态硬件检测与配置 ---
def get_optimal_execution_providers():
    """
    智能检测可用的ONNX Runtime执行提供者。
    优先使用CUDA，如果不可用则回退到CPU。
    """
    available_providers = onnxruntime.get_available_providers()
    if 'CUDAExecutionProvider' in available_providers:
        print("Hardware Check: GPU (CUDAExecutionProvider) is available. Using GPU for inference.")
        return ['CUDAExecutionProvider', 'CPUExecutionProvider']
    else:
        print("Hardware Check: GPU (CUDA) not available. Falling back to CPUExecutionProvider.")
        return ['CPUExecutionProvider']

# --- 感知层配置 ---
MODEL_PATH = "models/dino_best.onnx"
# 动态获取最佳的执行配置
ORT_EXECUTION_PROVIDERS = get_optimal_execution_providers() 
# 置信度阈值
CONFIDENCE_THRESHOLD = 0.6

# --- 世界建模层配置 ---
GAME_ROI = (0, 0, 800, 600) 

# --- 决策规划层配置 ---
JUMP_TRIGGER_DISTANCE = 150

# --- 动作执行层配置 ---
ACTION_COOLDOWN = 0.1
----------------------------------------------------

--- 文件: C:\dino_ai\src\contracts.py ---

# src/contracts.py
from dataclasses import dataclass, field
from typing import List, Tuple
from enum import Enum

# 动作契约 (Action Contract)
class Action(Enum):
    """定义了所有可能的AI动作，用于决策层与执行层的解耦。"""
    NOOP = 0  # No-Operation, 无操作
    JUMP = 1
    DUCK = 2

# 感知层契约 (Perception Contract)
@dataclass(frozen=True)
class Detection:
    """定义了单个检测到的物体，由感知层输出。"""
    label: str
    box: Tuple[int, int, int, int] # (x1, y1, x2, y2)
    confidence: float

# 世界建模层契约 (World Modeling Contract)
@dataclass(frozen=True)
class Obstacle:
    """定义了单个障碍物的完整状态，由世界模型计算。"""
    label: str
    box: Tuple[int, int, int, int]
    distance: float # 与恐龙的水平距离
    speed: float    # 估算出的水平速度

@dataclass(frozen=True)
class WorldState:
    """定义了某一帧的完整世界状态，是世界模型的核心输出。"""
    is_valid: bool # 状态是否有效（例如，是否检测到恐龙）
    dino_box: Tuple[int, int, int, int]
    # 总是返回一个列表，即使没有障碍物
    obstacles: List[Obstacle] = field(default_factory=list)

    @property
    def closest_obstacle(self) -> Obstacle | None:
        """提供一个方便的接口来获取最近的障碍物。"""
        if not self.obstacles:
            return None
        return min(self.obstacles, key=lambda o: o.distance)
----------------------------------------------------

--- 文件: C:\dino_ai\src\controls\__init__.py ---


----------------------------------------------------

--- 文件: C:\dino_ai\src\controls\agent.py ---

# src/controls/agent.py (V4 - 状态感知版 & 增强注释)
import time
from pynput.keyboard import Controller, Key

class GameAgent:
    """
    动作执行层：负责将决策规划层输出的抽象动作（如“跳跃”、“下蹲”），
    转化为对游戏窗口的具体键盘输入。
    """
    def __init__(self):
        # last_action_time: 记录上一次动作的时间戳
        self.last_action_time = 0
        
        # is_busy 和 busy_until_time: 用于实现一个简单的动作冷却或状态锁定。
        # 在L2阶段，这用于防止行为树在一个动作未完成时触发下一个。
        # 在L3阶段，这可以作为一个可选的保护机制，防止决策模型过于“抽搐”。
        # 当前在 run_bot.py 中，我们没有主动调用 check_busy()，
        # 所以这个机制目前是“休眠”的。
        self.is_busy = False 
        self.busy_until_time = 0 
        
        # 初始化 pynput 的键盘控制器
        self.keyboard = Controller()

    def _update_busy_state(self, duration):
        """
        内部方法：更新agent的忙碌状态和时间。
        当一个动作被执行时，将agent标记为“忙碌”，直到指定的持续时间结束。
        """
        self.is_busy = True
        self.last_action_time = time.time()
        self.busy_until_time = self.last_action_time + duration

    def check_busy(self):
        """
        外部检查agent是否忙碌。如果忙碌时间已过，则重置状态。
        """
        if self.is_busy and time.time() > self.busy_until_time:
            self.is_busy = False 
        return self.is_busy

    def jump(self, duration=0.05):
        """
        执行“跳跃”动作。
        模拟按下并释放空格键。

        Args:
            duration (float): 按下空格键的持续时间（秒）。
        """
        # 跳跃动作的总持续时间，可以略长于按键时间，给一个落地缓冲。
        # 这主要用于 _update_busy_state，以定义一个合理的“冷却时间”。
        JUMP_TOTAL_DURATION = 0.45 
        self._update_busy_state(JUMP_TOTAL_DURATION)
        
        print(f"[Action] JUMP triggered.") # 增加诊断打印
        self.keyboard.press(Key.space)
        time.sleep(duration) # 按住空格键一小段时间
        self.keyboard.release(Key.space)
        print(f"[Action] JUMP completed (busy for {JUMP_TOTAL_DURATION}s).")

    def duck(self):
        """
        执行“下蹲”动作。
        模拟按下并释放向下箭头键。
        """
        DUCK_TOTAL_DURATION = 0.4
        self._update_busy_state(DUCK_TOTAL_DURATION)

        print(f"[Action] DUCK triggered.") # 增加诊断打印
        self.keyboard.press(Key.down)
        time.sleep(0.3) # 按住下箭头键
        self.keyboard.release(Key.down)
        print(f"[Action] DUCK completed (busy for {DUCK_TOTAL_DURATION}s).")
----------------------------------------------------

--- 文件: C:\dino_ai\src\perception\__init__.py ---


----------------------------------------------------

--- 文件: C:\dino_ai\src\perception\detector.py ---

# src/perception/detector.py (V16 - 预处理与后处理黄金标准版)

import torch
import cv2
import numpy as np
from torchvision import models, transforms
from PIL import Image
from torch.profiler import profile, schedule, tensorboard_trace_handler
import onnxruntime as ort 
from pathlib import Path 

# --- 导入 Ultralytics 官方辅助函数 ---
from ultralytics.utils.ops import non_max_suppression, scale_boxes

# --- 辅助函数：Letterbox 预处理 (来自研究报告，精确复现) ---
def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):
    # Resize and pad image while meeting stride-multiple constraints
    shape = im.shape[:2]  # current shape [height, width] (H, W)
    if isinstance(new_shape, int):
        new_shape = (new_shape, new_shape) # new_shape is (W, H)

    # Scale ratio (new / old)
    # new_shape[0] is target width, shape[1] is current width
    # new_shape[1] is target height, shape[0] is current height
    r = min(new_shape[0] / shape[1], new_shape[1] / shape[0])
    if not scaleup:  # only scale down, do not scale up (for better val mAP)
        r = min(r, 1.0)

    # Compute padding
    # new_unpad is (width, height)
    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))

    # dw, dh are padding amounts for width and height
    # new_shape[0] is target width, new_unpad[0] is current unpadded width
    dw, dh = new_shape[0] - new_unpad[0], new_shape[1] - new_unpad[1]  
    
    if auto:  # minimum rectangle
        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding
    elif not scaleup: # Added from official source, ensures padding is non-negative if not scaling up
        dw, dh = max(0, dw), max(0, dh)

    dw /= 2  # divide padding into 2 sides
    dh /= 2

    if shape[::-1] != new_unpad:  # resize (shape[::-1] is (width, height))
        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR) # new_unpad is (width, height)
    
    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
    return im, r, (dw, dh) # Return img, ratio (single float r), and pad (tuple (dw, dh))

# --- 辅助函数：图像预处理 (整合 Letterbox, RGB转换，归一化，CHW转换) (来自研究报告) ---
def preprocess_image_for_yolo(original_image, new_shape=(640, 640)):
    # original_image is expected to be BGR from OpenCV imread
    # 1. BGR 到 RGB 转换 (如果使用 OpenCV 读取图像)
    img_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

    # 2. 应用 letterbox 调整大小和填充
    # new_shape=(width, height) for letterbox
    img_padded, ratio, pad = letterbox(img_rgb, new_shape=new_shape, auto=False, scaleup=True) 
    
    # 3. 通道转置 (HWC 到 CHW) (高, 宽, 通道 -> 通道, 高, 宽)
    img_chw = img_padded.transpose((2, 0, 1))

    # 4. 归一化像素值从 0-255 到 0-1
    img_normalized = np.ascontiguousarray(img_chw, dtype=np.float32) # Ensure contiguous memory
    img_normalized /= 255.0  # Normalize to 0-1

    # 5. 添加批次维度 (1, C, H, W)
    img_final = np.expand_dims(img_normalized, 0)

    return img_final, ratio, pad # Return processed image, scale ratio, and padding

class AdvancedDetector:
    def __init__(self, yolo_model_path, classifier_model_path):
        try:
            if torch.cuda.is_available():
                self.device = 'cuda'
                print("✅ Hardware Check: GPU (CUDA) is available. Inference will run on GPU.")
            else:
                self.device = 'cpu'
                print("⚠️ Hardware Check: GPU (CUDA) not found. Inference will fall back to CPU.")

            print(f"🧠 正在加载 YOLO Detector 模型: {yolo_model_path}")
            
            yolo_session_options = ort.SessionOptions()
            yolo_session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL

            yolo_trt_provider_options = {
                "trt_fp16_enable": True,
                "trt_cuda_graph_enable": False, # 暂时禁用 CUDA Graph，因为未实现 I/O Binding
                "trt_engine_cache_enable": True,
                "trt_engine_cache_path": str(Path(yolo_model_path).parent / "onnx_cache"),
                "trt_max_workspace_size": 1073741824, # 1GB 显存工作区
            }

            yolo_providers = [
                ("TensorrtExecutionProvider", yolo_trt_provider_options),
                "CUDAExecutionProvider",
                "CPUExecutionProvider",
            ]
            
            self.yolo_ort_session = ort.InferenceSession(
                yolo_model_path,
                sess_options=yolo_session_options,
                providers=yolo_providers
            )
            (Path(yolo_model_path).parent / "onnx_cache").mkdir(parents=True, exist_ok=True)
            print(f"✅ YOLO Detector ONNX Runtime Session 加载成功。使用设备: {self.yolo_ort_session.get_providers()}")
            print("注意：YOLO Detector第一次运行可能较慢，TensorRT正在构建优化引擎并缓存。")
            
        except Exception as e:
            print(f"❌ YOLO Detector: 模型加载失败。错误详情: {e}") 
            raise

        try:
            checkpoint = torch.load(classifier_model_path, map_location=self.device)
            self.class_names_classifier = checkpoint['class_names']
            num_classes = len(self.class_names_classifier)
            
            self.classifier_model = models.efficientnet_v2_s()
            num_ftrs = self.classifier_model.classifier[1].in_features
            self.classifier_model.classifier[1] = torch.nn.Linear(num_ftrs, num_classes)
            
            self.classifier_model.load_state_dict(checkpoint['model_state_dict'])
            self.classifier_model = self.classifier_model.to(self.device)
            self.classifier_model.eval()
            print(f"✅ Classifier: 模型 '{classifier_model_path}' 加载成功。")
            print(f"   -> 分类器类别: {self.class_names_classifier}")
            
            self.target_size = 224
            # --- [核心修改 1] 修正 classifier_transform 中 letterbox 的引用 ---
            # letterbox 现在是文件顶部的独立函数，而不是类方法
            self.classifier_transform = transforms.Compose([
                # 直接调用文件顶部的 letterbox 函数，而不是 self.letterbox
                transforms.Lambda(lambda img: letterbox(img, new_shape=(self.target_size, self.target_size), auto=False, scaleup=False)[0]), # letterbox返回(img, ratio, pad)，我们需要img
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
            # --------------------------------------------------------------------
        except Exception as e:
            print(f"❌ Classifier: 模型加载失败: {e}")
            raise
            
        self.last_obstacle_rois = []
        self.profiler = None

    def enable_profiler(self, log_dir="runs/profiler_logs"):
        """在需要时手动调用以启用profiler"""
        if self.profiler is None:
            sch = schedule(wait=20, warmup=10, active=10, repeat=1)
            self.profiler = profile(
                schedule=sch,
                on_trace_ready=tensorboard_trace_handler(log_dir),
                activities=[
                    torch.profiler.ProfilerActivity.CPU,
                    torch.profiler.ProfilerActivity.CUDA,
                ],
                record_shapes=True,
                profile_memory=True,
                with_stack=True 
            )
            self.profiler.start()
            print(f"PyTorch Profiler started, logs will be saved to {log_dir}")

    def disable_profiler(self):
        """在诊断结束后手动调用以禁用profiler"""
        if self.profiler is not None:
            self.profiler.stop()
            print(self.profiler.key_averages().table(sort_by="cuda_time_total", row_limit=10)) 
            print("PyTorch Profiler stopped.")
            self.profiler = None
            
    def _postprocess_yolo8_output(self, output, original_img_shape, ratio, pad, 
                                  conf_thres=0.01, iou_thres=0.45, yolo_class_names=None): # 默认置信度改为 0.01
        """
        后处理YOLOv8 ONNX (1,7,8400) 原始输出
        output: (1,7,8400) ONNX原始输出 (NumPy array)
        original_img_shape: (height, width) 原始图像尺寸
        ratio: Letterbox 预处理中的缩放比例
        pad: (dw, dh) Letterbox 预处理中的填充量
        """
        if yolo_class_names is None:
            yolo_class_names = ['bird', 'cactus', 'dino'] 

        # 1. 转置并移除批次维度：从 (1, 7, 8400) 到 (8400, 7)
        predictions = np.squeeze(output).T 

        # 2. 分离组件和计算置信度 (报告指出：YOLOv8输出已包含最终置信度，无需额外obj_conf * class_score)
        boxes_xywh_raw = predictions[:, :4]  # x, y, w, h (归一化，相对 640x640 letterbox图像)
        class_scores_raw = predictions[:, 4:] # 类别分数 (report says 4th index onwards are class confs)

        # 找到最高分数的类别ID和其置信度
        max_class_scores = np.max(class_scores_raw, axis=1) # 这就是最终置信度
        class_ids_raw = np.argmax(class_scores_raw, axis=1)

        raw_scores_sorted_idx = np.argsort(max_class_scores)[::-1] 
        top_k_print = min(10, len(raw_scores_sorted_idx)) 
        
        if top_k_print > 0:
            top_k_conf_scores = max_class_scores[raw_scores_sorted_idx[:top_k_print]]
            top_k_class_ids = class_ids_raw[raw_scores_sorted_idx[:top_k_print]]
            
            top_k_class_names = []
            for cid in top_k_class_ids:
                if cid >= 0 and cid < len(yolo_class_names): 
                    top_k_class_names.append(yolo_class_names[cid])
                else:
                    top_k_class_names.append(f"unknown_cls_{cid}")

            print(f"DEBUG: Top-{top_k_print} Raw Pred Confidences: {top_k_conf_scores.round(3)}, Class_IDs: {top_k_class_ids}, Names: {top_k_class_names}")
            if np.max(top_k_conf_scores) < conf_thres:
                print(f"DEBUG: 最高原始置信度 ({np.max(top_k_conf_scores):.3f}) 低于阈值 ({conf_thres:.3f})，可能无检测结果。")
        else:
            print("DEBUG: 无原始预测框。")

        # 5. 根据置信度阈值进行筛选
        keep_indices_conf = max_class_scores > conf_thres
        boxes_xywh_filtered = boxes_xywh_raw[keep_indices_conf]
        max_class_scores_filtered = max_class_scores[keep_indices_conf]
        class_ids_filtered = class_ids_raw[keep_indices_conf]

        if len(boxes_xywh_filtered) == 0:
            print(f"DEBUG: 经置信度 {conf_thres} 筛选后，无有效检测框。") 
            return [], [], []

        # 6. 应用非极大值抑制 (NMS) - 使用 Ultralytics 官方 NMS
        # 将 NumPy 数组转换为 PyTorch Tensor for NMS
        boxes_xyxy_tensor = torch.from_numpy(boxes_xywh_filtered).float() # (M, 4) in xywh format
        scores_tensor = torch.from_numpy(max_class_scores_filtered).float() # (M,)

        # Convert xywh to xyxy for NMS (torchvision.ops.nms or ultralytics.utils.ops.non_max_suppression expects xyxy)
        # Note: ultralytics non_max_suppression expects (batch_size, num_detections, 6) or (batch_size, num_detections, 4+num_classes)
        # The structure is [x1, y1, x2, y2, conf, class_id] after non_max_suppression.
        # So we need to format it before passing to non_max_suppression.
        # Let's rebuild the input to non_max_suppression to match its expectation.

        # Predictions for NMS need to be (N, 6) or (N, 4+num_classes) for NMS
        # Let's make it (N, 6) -> [x1, y1, x2, y2, conf, class_id]
        
        # Convert xywh to xyxy (normalized to 640x640 space) for NMS
        # This is where non_max_suppression expects its boxes
        
        # We need to construct a tensor of shape (N, 6) from our filtered data
        # [x, y, w, h, max_class_score, class_id]
        
        # Convert xywh to xyxy
        x1 = boxes_xywh_filtered[:, 0] - boxes_xywh_filtered[:, 2] / 2
        y1 = boxes_xywh_filtered[:, 1] - boxes_xywh_filtered[:, 3] / 2
        x2 = boxes_xywh_filtered[:, 0] + boxes_xywh_filtered[:, 2] / 2
        y2 = boxes_xywh_filtered[:, 1] + boxes_xywh_filtered[:, 3] / 2

        # Combine into (N, 6) tensor for NMS
        # (x1, y1, x2, y2, conf, class_id)
        nms_input = np.concatenate((x1[:, np.newaxis], y1[:, np.newaxis], 
                                    x2[:, np.newaxis], y2[:, np.newaxis], 
                                    max_class_scores_filtered[:, np.newaxis], 
                                    class_ids_filtered[:, np.newaxis]), axis=1)
        
        nms_input_tensor = torch.from_numpy(nms_input).float()
        
        # NMS 返回一个张量列表，批次中的每张图像对应一个张量。
        # 每个张量包含检测到的对象: [x1, y1, x2, y2, conf, class_id]
        results_from_nms = non_max_suppression(
            nms_input_tensor.unsqueeze(0), # Add batch dimension for NMS input (1, N, 6)
            conf_thres=conf_thres,         # 使用传入的 conf_thres
            iou_thres=iou_thres,           # 使用传入的 iou_thres
            classes=None,                  # 不按特定类别过滤
            agnostic=False,                # 不进行类别无关NMS
            max_det=1000                   # 每张图像的最大检测数量
        )
        
        if not results_from_nms or len(results_from_nms[0]) == 0: # NMS 后无结果
            print(f"DEBUG: 经 NMS (IoU={iou_thres}, Conf={conf_thres}) 后，无有效检测框。") 
            return [], [], []

        # results_from_nms[0] 是一个 PyTorch Tensor，形状为 (num_final_detections, 6)
        final_detections_tensor = results_from_nms[0] 

        # 转换为 NumPy 数组
        final_detections_np = final_detections_tensor.cpu().numpy()

        # 提取 NMS 后的最终框、置信度和类别ID
        final_boxes_xyxy_640 = final_detections_np[:, :4] # 框在 640x640 letterbox 空间
        final_scores_after_nms = final_detections_np[:, 4]
        final_class_ids_after_nms = final_detections_np[:, 5].astype(int)

        # 8. 精确坐标反变换：从 640x640 letterbox 图像映射回原始图像像素
        # 使用 Ultralytics 官方的 scale_boxes 函数
        # original_img_shape is (H, W)
        # new_shape is (W, H)
        # scale_boxes expects (img1_shape, boxes, img0_shape) for scaling boxes from img1 to img0
        # img1_shape: (H, W) of the image where boxes are currently defined (640, 640)
        # img0_shape: (H, W) of the target image (original_image_h, original_image_w)

        # Corrected: scale_boxes expects input boxes to be in (x1, y1, x2, y2) format already
        # And expects img1_shape (H,W) and img0_shape (H,W)
        
        # Convert final_boxes_xyxy_640 (W,H) format to (H,W) for scale_boxes
        # (640,640) is the input_shape to the model which is (W,H) in letterbox function, so it matches.
        
        # Scale boxes back to original image size
        # scale_boxes expects (img1_shape, boxes, img0_shape)
        # img1_shape is (H, W) where boxes are currently defined (640, 640)
        # img0_shape is (H, W) of the target image (original_image_h, original_image_w)
        # The boxes are already x1y1x2y2 format in 640x640 space
        
        # Use new_shape as (H,W) for scale_boxes for clarity
        model_input_hw = (640, 640) # YOLOv8 model input H, W (after letterbox)

        final_boxes_original_scaled = scale_boxes(
            model_input_hw, # Source shape (H,W)
            torch.from_numpy(final_boxes_xyxy_640).float(), # Boxes in source shape
            original_img_shape # Target shape (H,W)
        ).numpy()

        # 确保坐标是整数
        final_boxes_original_scaled = final_boxes_original_scaled.astype(int)

        final_boxes_processed = final_boxes_original_scaled
        final_scores = final_scores_after_nms
        final_class_ids = final_class_ids_after_nms
        
        return final_boxes_processed, final_scores, final_class_ids


    def detect(self, image, yolo_class_names, confidence_threshold=0.01): # 默认置信度改为 0.01 (用于调试)
        self.last_obstacle_rois = []
        final_detections = []
        obstacle_rois = []
        obstacle_boxes = []

        original_image_h, original_image_w, _ = image.shape 
        
        # --- [核心修改 1] 使用 preprocess_image_for_yolo 进行预处理 ---
        processed_img_tensor, ratio_scale, pad_amount = preprocess_image_for_yolo(image, new_shape=(640, 640)) # new_shape=(W,H) for letterbox input_size=640
        
        yolo_input_name = self.yolo_ort_session.get_inputs()[0].name
        
        raw_outputs = [] 
        try:
            # 显式请求输出名称 'output0' (根据脚本输出)
            raw_outputs = self.yolo_ort_session.run(['output0'], {yolo_input_name: processed_img_tensor})
        except Exception as e:
            print(f"❌ ERROR: YOLO ORT session.run failed during inference. Error: {e}")
            return [] 
        
        if not raw_outputs or not isinstance(raw_outputs[0], np.ndarray) or raw_outputs[0].size == 0:
            print(f"⚠️ 警告：YOLO模型原始输出结果为空或非数组。原始输出: {raw_outputs}。跳过检测。")
            return [] 

        # --- [核心修改 2] 调用 _postprocess_yolo8_output 进行后处理 ---
        # 传入原始图像尺寸、letterbox的 ratio 和 pad
        processed_boxes, processed_scores, processed_class_ids = self._postprocess_yolo8_output(
            raw_outputs[0], # yolo_outputs[0] 是 (1, 7, 8400) 的 NumPy 数组
            original_img_shape=(original_image_h, original_image_w), 
            ratio=ratio_scale, 
            pad=pad_amount, # pad_amount 是 (dw, dh)
            conf_thres=confidence_threshold, 
            iou_thres=0.45, 
            yolo_class_names=yolo_class_names
        )
        
        # 从 processed_boxes 中分离出 dino 和障碍物，并进行后续处理
        for i in range(len(processed_boxes)):
            box = processed_boxes[i]
            class_id = processed_class_ids[i]
            
            yolo_name_from_id = yolo_class_names[class_id] if class_id < len(yolo_class_names) else f"unknown_id_{class_id}"
            
            if yolo_name_from_id == 'dino':
                final_detections.append((box, 'dino-player'))
            elif yolo_name_from_id in ['cactus', 'bird']:
                x1, y1, x2, y2 = box
                padding = 10 
                h_img, w_img, _ = image.shape # image 是原始图像
                x1_pad = max(0, x1 - padding)
                y1_pad = max(0, y1 - padding)
                x2_pad = min(w_img, x2 + padding)
                y2_pad = min(h_img, y2 + padding) 

                roi = image[y1_pad:y2_pad, x1_pad:x2_pad]
                
                if roi.size > 0:
                    obstacle_rois.append(roi)
                    obstacle_boxes.append(box) 
        
        if obstacle_rois:
            if self.profiler is not None:
                self.profiler.step()
            
            self.last_obstacle_rois = obstacle_rois
            batch_tensor = torch.stack([self.classifier_transform(roi) for roi in obstacle_rois]).to(self.device)
            
            with torch.no_grad():
                outputs = self.classifier_model(batch_tensor)
                _, preds = torch.max(outputs, 1)
            
            for i, pred_idx in enumerate(preds):
                specific_class_name = self.class_names_classifier[pred_idx.item()]
                final_detections.append((obstacle_boxes[i], specific_class_name))
                
        return final_detections
----------------------------------------------------

--- 文件: C:\dino_ai\src\state_builder.py ---

# src/state_builder.py (V2 - 黄金标准校验版 & 增强注释)
import numpy as np

def normalize(value, min_val, max_val):
    """
    将一个值归一化到 [0, 1] 区间。
    """
    # 防止除以零
    if (max_val - min_val) == 0: 
        return 0.0
    # 使用 clip 确保结果在 [0, 1] 范围内
    return np.clip((value - min_val) / (max_val - min_val), 0.0, 1.0)

def build_state_vector(game_state, world_model):
    """
    构建决策模型所需的状态向量。
    该向量的维度和每个维度的含义，必须与训练决策模型时完全一致。

    当前状态向量 (7个维度):
    [
        0: 游戏速度 (归一化),
        1: 最近障碍物与恐龙的距离 (归一化),
        2: 最近障碍物的高度 (归一化),
        3: 最近障碍物的宽度 (归一化),
        4: 第二近障碍物与恐龙的距离 (归一化),
        5: 第二近障碍物的高度 (归一化),
        6: 第二近障碍物的宽度 (归一化)
    ]
    """
    # --- [核心] 定义归一化的最大值，这些值应与训练时保持一致 ---
    MAX_SPEED = 1000  # 游戏最大速度的估计值
    MAX_DIST = 800    # 障碍物最大距离的估计值
    MAX_HEIGHT = 100  # 障碍物最大高度的估计值
    MAX_WIDTH = 100   # 障碍物最大宽度的估计值

    # 从 game_state 和 world_model 中获取原始信息
    dino_box = game_state.dino_box
    obstacles = game_state.obstacles
    _, speed = world_model.get_state()
    
    # 确保速度不为None
    current_speed = speed if speed is not None else 0
    
    # 初始化状态向量，第一个维度是游戏速度
    state = [normalize(current_speed, 0, MAX_SPEED)]
    
    # 如果没有检测到恐龙，则无法计算距离，返回 None 表示状态无效
    if dino_box is None:
        return None

    # 按x坐标对障碍物进行排序，以便我们能找到最近和第二近的
    obstacles.sort(key=lambda o: o[0][0])
    
    # 填充最近的两个障碍物信息
    for i in range(2):
        if i < len(obstacles):
            # 如果存在第 i 个障碍物
            obs_box, _ = obstacles[i]
            
            # 计算障碍物与恐龙右侧边缘的距离
            dist = obs_box[0] - dino_box[2]
            # 计算障碍物的高度和宽度
            height = obs_box[3] - obs_box[1]
            width = obs_box[2] - obs_box[0]
            
            # 将归一化后的特征添加到状态向量
            state.extend([
                normalize(dist, 0, MAX_DIST), 
                normalize(height, 0, MAX_HEIGHT), 
                normalize(width, 0, MAX_WIDTH)
            ])
        else:
            # 如果不存在第 i 个障碍物（例如，只有一个或没有障碍物）
            # 使用默认值填充，表示“没有障碍物”
            # [1.0, 0.0, 0.0] 表示：距离无穷大 (归一化为1), 高度为0, 宽度为0
            state.extend([1.0, 0.0, 0.0])
            
    # 将 state 列表转换为 float32 类型的 NumPy 数组
    return np.array(state, dtype=np.float32)
----------------------------------------------------

--- 文件: C:\dino_ai\src\utils\__init__.py ---


----------------------------------------------------

--- 文件: C:\dino_ai\src\utils\screen_manager.py ---

# src/utils/screen_manager.py (V4 - 彻底分离版)
import cv2
import mss
import numpy as np
from PIL import ImageGrab
import os # 用于文件操作

class ScreenManager:
    def __init__(self, sct_instance):
        self.roi = None
        self.sct = sct_instance
        self.temp_screenshot_path = "temp_screenshot.png"

    def select_roi(self):
        """
        [核心修改] 彻底分离截图和GUI操作，根除死锁。
        """
        print("\n准备选择区域... 正在截取您的主屏幕...")
        
        # 步骤1：仅截图，并立刻保存到文件
        try:
            full_screenshot = ImageGrab.grab()
            full_screenshot.save(self.temp_screenshot_path)
            print("截图已保存。")
        except Exception as e:
            print(f"❌ 截图失败: {e}")
            self.roi = None
            return

        # 步骤2：从文件加载静态图片，再进行GUI操作
        try:
            img_from_file = cv2.imread(self.temp_screenshot_path)
            if img_from_file is None:
                raise FileNotFoundError("无法读取保存的截图文件。")

            window_name = "请用鼠标拖动选择Dino游戏区域, 然后按 ENTER 或 SPACE 键确认"
            print(f"在弹出的 '{window_name}' 窗口中操作...")
            
            cv2.namedWindow(window_name, cv2.WND_PROP_FULLSCREEN)
            cv2.setWindowProperty(window_name, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)
            
            roi_coords = cv2.selectROI(window_name, img_from_file, fromCenter=False, showCrosshair=True)
            cv2.destroyAllWindows()
            
            # 步骤3：清理临时文件
            os.remove(self.temp_screenshot_path)

            if sum(roi_coords) == 0:
                print("❌ ROI selection cancelled.")
                self.roi = None
                return
                
            x, y, w, h = roi_coords
            self.roi = {"top": y, "left": x, "width": w, "height": h}
            print(f"✅ 游戏区域选择成功: {self.roi}")
        
        except Exception as e:
            print(f"❌ 选择区域时发生错误: {e}")
            self.roi = None
            if os.path.exists(self.temp_screenshot_path):
                os.remove(self.temp_screenshot_path) # 确保清理
            return
            
    def capture(self) -> np.ndarray:
        if not self.roi:
            return None
        
        sct_img = self.sct.grab(self.roi)
        return cv2.cvtColor(np.array(sct_img), cv2.COLOR_BGRA2BGR)
----------------------------------------------------

--- 文件: C:\dino_ai\src\world_modeling\__init__.py ---


----------------------------------------------------

--- 文件: C:\dino_ai\src\world_modeling\state.py ---

# src/world_modeling/state.py (V5 - 感知时间)

class GameState:
    def __init__(self):
        self.dino_box = None
        self.obstacles = []
        self.dt = 1/30 # 默认值

    def update(self, detections, dt): # 接收dt
        self.dino_box = None
        self.obstacles = []
        self.dt = dt # 保存真实的dt
        
        for box, class_name in detections:
            if class_name == 'dino-player':
                self.dino_box = box
            else:
                self.obstacles.append((box, class_name))
----------------------------------------------------

--- 文件: C:\dino_ai\src\world_modeling\world_model.py ---

# src/world_modeling/world_model.py (V3 - 增强诊断与注释)
import numpy as np
from filterpy.kalman import UnscentedKalmanFilter
from filterpy.kalman import MerweScaledSigmaPoints
import time

class UKFWorldModel:
    """
    世界建模层：使用无迹卡尔曼滤波器 (Unscented Kalman Filter, UKF)
    来估计最接近障碍物的状态（位置和速度）。
    这是 L2/L3 级智能体中，从纯粹的“感知”迈向“理解”的关键一步。
    """
    def __init__(self): 
        self.tracker = None

    def _create_tracker(self, initial_x, dt):
        """
        初始化UKF追踪器。
        状态向量 (ukf.x): [position, velocity]
        观测向量 (ukf.z): [position]
        """
        # MerweScaledSigmaPoints 是一种用于UKF的采样点生成策略
        points = MerweScaledSigmaPoints(n=2, alpha=0.1, beta=2.0, kappa=-1)
        
        # dim_x=2: 状态向量维度为2 (位置, 速度)
        # dim_z=1: 观测向量维度为1 (只观测位置)
        ukf = UnscentedKalmanFilter(dim_x=2, dim_z=1, dt=dt, hx=self._hx, fx=self._fx, points=points)

        # 初始状态：位置为首次观测到的位置，速度为0
        ukf.x = np.array([initial_x, 0.0])
        
        # 状态协方差矩阵 (P): 表示状态估计的不确定性。
        # 位置不确定性较小，速度不确定性较大。
        ukf.P = np.diag([100.0, 5000.0]) 
        
        # 测量噪声协方差 (R): 表示我们对观测值的信任程度。值越小，越信任观测。
        ukf.R = np.diag([25.0]) 
        
        # 过程噪声协方差 (Q): 表示我们对状态转移模型(_fx)的信任程度。
        # 值越大，表示我们认为系统动态变化越快，对模型的预测越不信任。
        ukf.Q = np.array([[0.1, 0.0], [0.0, 10.0]])
        
        return ukf

    def _fx(self, x, dt):
        """
        状态转移函数：根据物理模型预测下一时刻的状态。
        这里使用简单的匀速运动模型。
        pos_next = pos + vel * dt
        vel_next = vel
        """
        F = np.array([[1, dt], [0, 1]])
        return F @ x

    def _hx(self, x):
        """
        观测函数：将状态向量映射到观测空间。
        我们只能观测到位置，所以只返回状态向量的第一个元素。
        """
        return np.array([x[0]])

    def update(self, obstacle_measurement, dt):
        """
        用新的观测数据更新世界模型。
        """
        # 如果没有检测到障碍物，重置追踪器。
        # 这是为了防止在障碍物消失后，模型继续错误地预测其位置。
        if obstacle_measurement is None:
            self.tracker = None
            return

        # 我们只关心障碍物的x坐标
        x_pos = obstacle_measurement[0][0]

        # 如果追踪器未初始化，则用当前观测值创建新的追踪器。
        if self.tracker is None:
            self.tracker = self._create_tracker(x_pos, dt)
        
        # UKF标准步骤：预测 -> 更新
        self.tracker.predict(dt=dt)
        self.tracker.update(np.array([x_pos]))
        
    def get_state(self):
        """

        获取当前世界模型的状态（位置和速度）。
        """
        if self.tracker is None:
            return None, None
        
        pos = self.tracker.x[0]
        # 速度取负值，因为障碍物在屏幕上向左移动（x坐标减小），
        # 但我们通常将游戏“速度”理解为一个正值。
        vel = -self.tracker.x[1] 
        return pos, vel
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\1_detection_pipeline\train_detector.py ---

import sys
from pathlib import Path
from ultralytics import YOLO
import torch

# [核心修改] 让脚本能够感知到项目的根目录
# 这使得无论我们从哪里运行这个脚本，它都能正确找到其他文件
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

def main():
    """
    训练并导出YOLOv8目标检测模型。
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # --- [核心修改] 使用绝对路径，确保路径的稳健性 ---
    # 定义所有输入文件的路径
    base_model_path = project_root / "_inputs" / "base_models" / "yolov8n.pt"
    data_config_path = project_root / "training_pipelines" / "1_detection_pipeline" / "data.yaml"
    
    # 定义所有输出的根目录
    output_dir = project_root / "training_runs"
    
    print(f"加载基础模型: {base_model_path}")
    print(f"加载数据配置: {data_config_path}")
    print(f"训练产出将保存至: {output_dir}")

    # 1. 加载预训练模型
    model = YOLO(base_model_path)

    # 2. 训练模型
    print("开始目标检测模型训练...")
    model.train(
        data=str(data_config_path), # 确保传入的是字符串
        epochs=50,
        imgsz=640,
        batch=8,
        project=str(output_dir), # [核心修改] 控制输出目录
        name='detection_run'     # 定义本次实验的名称
    )
    print("训练完成。")

    # 3. 导出模型为ONNX
    # 加载训练出的最佳模型
    best_model_path = model.trainer.best
    model = YOLO(best_model_path)
    
    print(f"正在从 {best_model_path} 导出最佳模型为ONNX...")
    # 导出的ONNX文件会自动保存在与 best.pt 相同的目录下
    model.export(format='onnx', opset=12) 
    print("导出完成。")
    print(f"请手动将最终的 .onnx 文件移动到 'models/detection/' 目录下，并重命名为 'dino_detector.onnx'。")


if __name__ == '__main__':
    main()
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\2_classification_pipeline\1_generate_dataset.py ---

import sys
from pathlib import Path
import os
import random
from PIL import Image

# [核心修改] 让脚本能够感知到项目的根目录
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [核心修改] 使用绝对路径定义输入和输出 ---
CONFIG = {
    "assets_path": project_root / "_assets",
    "output_path": project_root / "data" / "classification_data",
    "image_size": (100, 100),
    "background_color": (247, 247, 247),
    "images_per_class": 500,
    "min_scale": 0.7,
    "max_scale": 1.0,
}

def generate_classification_dataset():
    """
    [核心修正] 直接、忠实地使用 _assets 文件夹中的每一个原始 png 文件来生成数据。
    """
    assets_path = Path(CONFIG["assets_path"])
    output_path = Path(CONFIG["output_path"])

    # 确保输出目录存在
    output_path.mkdir(exist_ok=True, parents=True)

    print(f"输入素材目录: {assets_path}")
    print(f"输出数据集目录: {output_path}")

    if not assets_path.exists():
        print(f"❌ 错误：资产文件夹 '{assets_path}' 不存在！")
        return

    asset_files = list(assets_path.glob("*.png"))
    if not asset_files:
        print(f"❌ 错误：在 '{assets_path}' 中没有找到任何.png文件！")
        return

    print(f"🔍 找到了 {len(asset_files)} 个资产文件。开始生成数据集...")

    for asset_file in asset_files:
        if 'dino-player' in asset_file.stem:
            print(f"  -> 跳过非障碍物素材: {asset_file.name}")
            continue

        class_name = asset_file.stem
        class_dir = output_path / class_name
        class_dir.mkdir(parents=True, exist_ok=True)

        print(f"  -> 正在为类别 '{class_name}' 生成 {CONFIG['images_per_class']} 张图片...")
        
        try:
            asset_img = Image.open(asset_file).convert("RGBA")
        except Exception as e:
            print(f"   ⚠️ 警告：无法打开文件 {asset_file}，已跳过。错误: {e}")
            continue

        for i in range(CONFIG["images_per_class"]):
            background = Image.new("RGBA", CONFIG["image_size"], CONFIG["background_color"])
            
            max_allowed_scale_w = CONFIG["image_size"][0] / asset_img.width
            max_allowed_scale_h = CONFIG["image_size"][1] / asset_img.height
            final_max_scale = min(max_allowed_scale_w, max_allowed_scale_h, CONFIG["max_scale"])
            
            if final_max_scale < CONFIG["min_scale"]:
                scale = final_max_scale
            else:
                scale = random.uniform(CONFIG["min_scale"], final_max_scale)

            new_width = int(asset_img.width * scale)
            new_height = int(asset_img.height * scale)
            
            if new_width <= 0 or new_height <= 0: continue

            resized_asset = asset_img.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            max_x = CONFIG["image_size"][0] - new_width
            max_y = CONFIG["image_size"][1] - new_height
            
            paste_x = random.randint(0, max(0, max_x))
            paste_y = random.randint(0, max(0, max_y))

            background.paste(resized_asset, (paste_x, paste_y), resized_asset)
            
            final_image = background.convert("RGB")
            output_file_path = class_dir / f"{i:04d}.png"
            final_image.save(output_file_path)

    print(f"\n✅ 数据集生成完毕！已保存至 '{output_path}' 目录。")

if __name__ == "__main__":
    generate_classification_dataset()
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\2_classification_pipeline\2_train_classifier.py ---

# training_pipelines/2_classification_pipeline/2_train_classifier.py (V1.2 - 最终修正版)
import sys
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, models, transforms
from torch.utils.data import DataLoader
import time
import copy # 导入copy模块

# [核心修改] 让脚本能够感知到项目的根目录
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [核心修改] 使用绝对路径定义输入和输出 ---
CONFIG = {
    "dataset_path": project_root / "data" / "classification_data",
    "output_model_dir": project_root / "models" / "classification",
    "output_model_name": "dino_classifier.pth",
    "num_epochs": 15,
    "batch_size": 32,
    "learning_rate": 0.001,
}

def train_classifier():
    """
    使用迁移学习训练一个EfficientNetV2模型来分类恐龙游戏中的障碍物。
    """
    print("开始训练图像分类器...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"使用设备: {device}")

    # 确保输出目录存在
    output_dir = Path(CONFIG["output_model_dir"])
    output_dir.mkdir(exist_ok=True, parents=True)

    # 1. 数据预处理和加载
    data_transforms = {
        'train': transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    dataset_path = Path(CONFIG["dataset_path"])
    print(f"加载数据集: {dataset_path}")
    if not dataset_path.exists() or not any(dataset_path.iterdir()):
        print(f"❌ 错误: 数据集目录 '{dataset_path}' 不存在或为空!")
        print("请先运行 1_generate_dataset.py 来生成数据。")
        return
        
    full_dataset = datasets.ImageFolder(dataset_path)
    
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])

    # --- [核心修正] 使用深拷贝解决Subset共享transform的问题 ---
    val_dataset.dataset = copy.deepcopy(full_dataset)
    train_dataset.dataset.transform = data_transforms['train']
    val_dataset.dataset.transform = data_transforms['val']
    # ----------------------------------------------------

    dataloaders = {
        'train': DataLoader(train_dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=4),
        'val': DataLoader(val_dataset, batch_size=CONFIG["batch_size"], shuffle=False, num_workers=4)
    }
    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}
    class_names = full_dataset.classes
    num_classes = len(class_names)
    print(f"发现 {num_classes} 个类别: {class_names}")

    # 2. 加载预训练模型并修改最后一层
    model = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.DEFAULT)
    for param in model.parameters():
        param.requires_grad = False
        
    num_ftrs = model.classifier[1].in_features
    model.classifier[1] = nn.Linear(num_ftrs, num_classes)
    model = model.to(device)
    print("✅ EfficientNetV2 模型加载并修改完成。")

    # 3. 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.classifier.parameters(), lr=CONFIG["learning_rate"])

    # 4. 训练循环
    since = time.time()
    best_model_wts = model.state_dict()
    best_acc = 0.0

    for epoch in range(CONFIG["num_epochs"]):
        print(f'Epoch {epoch+1}/{CONFIG["num_epochs"]}')
        print('-' * 10)
        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            running_loss = 0.0
            running_corrects = 0
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]
            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = model.state_dict()
                print(f'🎉 新的最佳验证集准确率: {best_acc:.4f}')
    
    time_elapsed = time.time() - since
    print(f'训练完成，耗时 {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')
    print(f'最佳验证集准确率: {best_acc:4f}')
    
    # 5. 保存最佳模型
    model.load_state_dict(best_model_wts)
    save_path = output_dir / CONFIG["output_model_name"]
    torch.save({
        'model_state_dict': model.state_dict(),
        'class_names': class_names
    }, save_path)
    print(f"✅ 最佳模型已保存至: {save_path}")

if __name__ == '__main__':
    train_classifier()
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\3_policy_pipeline\1_collect_data.py ---

# training_pipelines/3_policy_pipeline/1_collect_data.py (V1.1 - 最终路径修正版)
import sys
from pathlib import Path
import cv2
import mss
import time
import numpy as np
import pynput
import uuid
import os # 导入os模块

# [核心] 修正Python模块搜索路径，确保无论从哪里运行都能找到src
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.perception.detector import AdvancedDetector
from src.world_modeling.state import GameState
from src.world_modeling.world_model import UKFWorldModel
from src.utils.screen_manager import ScreenManager

# --- [核心修正] 严格对齐最终目录结构 ---
RAW_DATA_DIR = project_root / "data" / "policy_data" / "raw"
RAW_DATA_DIR.mkdir(exist_ok=True, parents=True)


ACTION_MAP = {'nothing': np.array([1, 0, 0], dtype=np.float32), 'jump': np.array([0, 1, 0], dtype=np.float32), 'duck': np.array([0, 0, 1], dtype=np.float32)}
current_action = ACTION_MAP['nothing']
def on_press(key):
    global current_action
    if key == pynput.keyboard.Key.space: current_action = ACTION_MAP['jump']
    elif key == pynput.keyboard.Key.down: current_action = ACTION_MAP['duck']
def on_release(key):
    global current_action
    if key in [pynput.keyboard.Key.space, pynput.keyboard.Key.down]: current_action = ACTION_MAP['nothing']
def normalize(value, min_val, max_val):
    if (max_val - min_val) == 0: return 0.0
    return max(0.0, min(1.0, (value - min_val) / (max_val - min_val)))
def build_state_vector(game_state, world_model):
    dino_box = game_state.dino_box
    obstacles = game_state.obstacles
    _, speed = world_model.get_state()
    MAX_SPEED, MAX_DIST, MAX_HEIGHT, MAX_WIDTH = 1000, 800, 100, 100
    state = [normalize(speed if speed else 0, 0, MAX_SPEED)]
    if dino_box is None: return None
    obstacles.sort(key=lambda o: o[0][0])
    for i in range(2):
        if i < len(obstacles):
            obs_box, _ = obstacles[i]
            dist = obs_box[0] - dino_box[2]
            state.extend([normalize(dist, 0, MAX_DIST), normalize(obs_box[3] - obs_box[1], 0, MAX_HEIGHT), normalize(obs_box[2] - obs_box[0], 0, MAX_WIDTH)])
        else:
            state.extend([1.0, 0.0, 0.0])
    return np.array(state, dtype=np.float32)

def main():
    print("准备开始采集专家数据...")
    listener = pynput.keyboard.Listener(on_press=on_press, on_release=on_release)
    listener.start()
    with mss.mss() as sct:
        # --- [核心修正] 严格对齐最终模型路径 ---
        detector = AdvancedDetector(
            yolo_model_path=str(project_root / "models" / "detection" / "dino_detector.onnx"),
            classifier_model_path=str(project_root / "models" / "classification" / "dino_classifier.pth")
        )
        # --------------------------------------
        
        game_state, world_model, screen_manager = GameState(), UKFWorldModel(), ScreenManager(sct)
        screen_manager.select_roi()
        if screen_manager.roi is None: return

        print(f"开始采集... 数据将保存至 {RAW_DATA_DIR}")
        print("按 'q' 键停止。")
        prev_time = time.time()
        while True:
            current_time = time.time()
            dt = current_time - prev_time
            if dt == 0: dt = 1/60
            prev_time = current_time
            
            img = screen_manager.capture()
            if img is None: continue
            
            detections = detector.detect(img, yolo_class_names=['bird', 'cactus', 'dino'])
            game_state.update(detections, dt)
            
            closest_obs = min(game_state.obstacles, key=lambda o: o[0][0]) if game_state.obstacles else None
            world_model.update(closest_obs, dt)
            state_vector = build_state_vector(game_state, world_model)
            
            if state_vector is not None:
                filename = RAW_DATA_DIR / f"{uuid.uuid4()}.npz"
                np.savez(filename, state=state_vector, action=current_action)

            cv2.imshow("Data Collection", img)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    listener.stop()
    cv2.destroyAllWindows()
    print(f"数据采集完成！")

if __name__ == "__main__":
    main()
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\3_policy_pipeline\2_process_data.py ---

# training_pipelines/3_policy_pipeline/2_process_data.py
import sys
from pathlib import Path
import numpy as np
import os
from tqdm import tqdm

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [核心修改] 更新输入和输出目录 ---
RAW_DATA_DIR = project_root / "data" / "policy_data" / "raw"
PROCESSED_DATA_DIR = project_root / "data" / "policy_data" / "processed"
PROCESSED_DATA_DIR.mkdir(exist_ok=True, parents=True)
PROCESSED_DATA_PATH = PROCESSED_DATA_DIR / "iql_dataset.npz"

SURVIVAL_REWARD, CRASH_PENALTY, EPISODE_TIMEOUT_SECONDS = 0.1, -100.0, 2.0

def process_raw_data():
    print(f"🔍 开始从 {RAW_DATA_DIR} 读取原始数据文件...")
    files = sorted(list(RAW_DATA_DIR.glob("*.npz")), key=os.path.getmtime)
    if not files:
        print(f"❌ 错误：在 {RAW_DATA_DIR} 找不到 .npz 文件。请先运行 1_collect_data.py。")
        return
    print(f"✅ 找到了 {len(files)} 个数据点。")
    
    observations, actions, rewards, terminals, next_observations = [], [], [], [], []
    
    print("🔄 开始处理数据，构建转换序列 (s, a, r, s', d)...")
    for i in tqdm(range(len(files) - 1), desc="Processing Transitions"):
        current_data, next_data = np.load(files[i]), np.load(files[i+1])
        done = (os.path.getmtime(files[i+1]) - os.path.getmtime(files[i])) > EPISODE_TIMEOUT_SECONDS
        
        observations.append(current_data['state'])
        actions.append(np.argmax(current_data['action']))
        rewards.append(CRASH_PENALTY if done else SURVIVAL_REWARD)
        terminals.append(done)
        next_observations.append(next_data['state'])


    if files:
        last_data = np.load(files[-1])
        observations.append(last_data['state'])
        actions.append(np.argmax(last_data['action']))
        rewards.append(CRASH_PENALTY)
        terminals.append(True)
        next_observations.append(np.zeros_like(last_data['state']))
        
    print(f"💾 数据处理完成，正在保存到 {PROCESSED_DATA_PATH}...")
    np.savez(
        PROCESSED_DATA_PATH,
        observations=np.array(observations, dtype=np.float32),
        actions=np.array(actions, dtype=np.uint8),
        rewards=np.array(rewards, dtype=np.float32),
        terminals=np.array(terminals, dtype=np.float32),
        next_observations=np.array(next_observations, dtype=np.float32),
    )
    print("🎉 成功！IQL数据集已准备就绪。")

if __name__ == "__main__":
    process_raw_data()
----------------------------------------------------

--- 文件: C:\dino_ai\training_pipelines\3_policy_pipeline\3_train_and_export.py ---

# training_pipelines/3_policy_pipeline/3_train_and_export.py
import sys
from pathlib import Path
import numpy as np
import torch
from d3rlpy.algos import DiscreteCQLConfig
from d3rlpy.datasets import MDPDataset
from d3rlpy.logging import CombineAdapterFactory, FileAdapterFactory, TensorboardAdapterFactory
from d3rlpy.metrics import TDErrorEvaluator, AverageValueEstimationEvaluator

project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

# --- [核心修改] 更新所有路径 ---
DATASET_PATH = project_root / "data" / "policy_data" / "processed" / "iql_dataset.npz"
LOG_DIR_BASE = project_root / "training_runs" # 将所有训练日志统一存放
EXPERIMENT_NAME = "policy_run"
ONNX_MODEL_PATH = project_root / "models" / "policy" / "dino_policy.onnx"
ONNX_MODEL_PATH.parent.mkdir(exist_ok=True)

TRAIN_STEPS, STEPS_PER_EPOCH, SAVE_INTERVAL_IN_EPOCHS = 200000, 10000, 2

def main():
    print(f"🧠 开始执行“一键式”训练与导出流程...")
    
    # 1. 加载数据集
    try:
        data = np.load(DATASET_PATH)
        dataset = MDPDataset(
            observations=data['observations'], actions=data['actions'],
            rewards=data['rewards'], terminals=data['terminals'],
        )
        print(f"✅ 成功加载并构建数据集: {DATASET_PATH}")
    except FileNotFoundError:
        print(f"❌ 错误：找不到数据集文件 {DATASET_PATH}。请先运行 2_process_data.py。")
        return
        
    # 2. 配置 DiscreteCQL
    cql_config = DiscreteCQLConfig(
        batch_size=256, gamma=0.99, learning_rate=1e-4, alpha=1.0, n_critics=2,
    )

    # 3. 创建实例
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"💻 使用设备: {device}")
    cql = cql_config.create(device=device)

    # 4. 配置日志
    logger = CombineAdapterFactory([
        FileAdapterFactory(root_dir=str(LOG_DIR_BASE)),
        TensorboardAdapterFactory(root_dir=str(LOG_DIR_BASE))
    ])
    
    # 5. 配置评估器
    evaluators = { 'td_error': TDErrorEvaluator(), 'avg_value': AverageValueEstimationEvaluator() }
    
    # 6. 开始训练
    print("🚀 正在启动训练... 你可以通过以下命令启动TensorBoard来实时监控：")
    print(f"   tensorboard --logdir {LOG_DIR_BASE}")
    
    cql.fit(
        dataset, n_steps=TRAIN_STEPS, n_steps_per_epoch=STEPS_PER_EPOCH,
        save_interval=SAVE_INTERVAL_IN_EPOCHS, logger_adapter=logger,
        experiment_name=EXPERIMENT_NAME, with_timestamp=False, evaluators=evaluators,
    )
    
    # 7. 导出最终模型
    final_model_dir = LOG_DIR_BASE / EXPERIMENT_NAME
    print(f"💾 训练完成，正在将最终策略导出为 ONNX 格式到 {ONNX_MODEL_PATH}...")
    cql.save_policy(str(ONNX_MODEL_PATH))
    
    print("\n" + "="*50)
    print("🎉🎉🎉 “一键式”流程执行完毕！你的最优专家大脑已成功铸造！ 🎉🎉🎉")
    print(f"最终产出: {ONNX_MODEL_PATH}")
    print("现在，我们可以满怀信心地去运行最终的 run_bot.py 了！")
    print("="*50)

if __name__ == "__main__":
    main()
----------------------------------------------------

====================================================
  [AI 分析数据结束]
====================================================
